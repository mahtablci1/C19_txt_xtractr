{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, \"../\")\n",
    "\n",
    "root_path = '.../Desktop/kaggle/input/corona_challenge/'\n",
    "\n",
    "corona_features = {\"doc_id\": [None], \"source\": [None], \"title\": [None],\n",
    "                  \"abstract\": [None], \"text_body\": [None]}\n",
    "corona_df = pd.DataFrame.from_dict(corona_features)\n",
    "\n",
    "json_filenames = glob.glob(f'{root_path}/**/*.json', recursive=True)\n",
    "# from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- most frequent unigram and bigram\n",
    "- Using topic modeling to see if we can find relevant paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_corona_df(json_filenames, df, source):\n",
    "\n",
    "    for file_name in json_filenames:\n",
    "\n",
    "        row = {\"doc_id\": None, \"source\": None, \"title\": None,\n",
    "              \"abstract\": None, \"text_body\": None}\n",
    "\n",
    "        with open(file_name) as json_data:\n",
    "            data = json.load(json_data)\n",
    "\n",
    "            doc_id = data['paper_id']\n",
    "            row['doc_id'] = doc_id\n",
    "            row['title'] = data['metadata']['title']\n",
    "\n",
    "            # Now need all of abstract. Put it all in \n",
    "            # a list then use str.join() to split it\n",
    "            # into paragraphs. \n",
    "\n",
    "            abstract_list = [abst['text'] for abst in data['abstract']]\n",
    "            abstract = \"\\n \".join(abstract_list)\n",
    "\n",
    "            row['abstract'] = abstract\n",
    "\n",
    "            # And lastly the body of the text. \n",
    "            body_list = [bt['text'] for bt in data['body_text']]\n",
    "            body = \"\\n \".join(body_list)\n",
    "            \n",
    "            row['text_body'] = body\n",
    "            \n",
    "            # Now just add to the dataframe. \n",
    "            \n",
    "            if source == 'b':\n",
    "                row['source'] = \"BIORXIV\"\n",
    "            elif source == \"c\":\n",
    "                row['source'] = \"COMMON_USE_SUB\"\n",
    "            elif source == \"n\":\n",
    "                row['source'] = \"NON_COMMON_USE\"\n",
    "            elif source == \"p\":\n",
    "                row['source'] = \"PMC_CUSTOM_LICENSE\"\n",
    "            \n",
    "            df = df.append(row, ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "corona_df = return_corona_df(json_filenames, corona_df, 'p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corona_out = corona_df.to_csv('.../Desktop/kaggle/output/corona_challenge/PMC_CUSTOM_LICENSE.csv')\n",
    "root_path = '.../Desktop/kaggle/output/corona_challenge/'\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in glob.glob(f'{root_path}/**/*.csv', recursive=True)]\n",
    "all_4 = pd.concat([pd.read_csv(f) for f in all_filenames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('.../Desktop/kaggle/metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.concat([all_4,metadata])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sha</th>\n",
       "      <th>source_x</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>Microsoft Academic Paper ID</th>\n",
       "      <th>WHO #Covidence</th>\n",
       "      <th>has_full_text</th>\n",
       "      <th>full_text_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>Intrauterine virus infections and congenital h...</td>\n",
       "      <td>10.1016/0002-8703(72)90077-4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4361535.0</td>\n",
       "      <td>els-covid</td>\n",
       "      <td>Abstract The etiologic basis for the vast majo...</td>\n",
       "      <td>1972-12-31</td>\n",
       "      <td>Overall, James C.</td>\n",
       "      <td>American Heart Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>custom_license</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>Coronaviruses in Balkan nephritis</td>\n",
       "      <td>10.1016/0002-8703(80)90355-5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6243850.0</td>\n",
       "      <td>els-covid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1980-03-31</td>\n",
       "      <td>Georgescu, Leonida; Diosi, Peter; Bu≈£iu, Ioan;...</td>\n",
       "      <td>American Heart Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>custom_license</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>Cigarette smoking and coronary heart disease: ...</td>\n",
       "      <td>10.1016/0002-8703(80)90356-7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7355701.0</td>\n",
       "      <td>els-covid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1980-03-31</td>\n",
       "      <td>Friedman, Gary D</td>\n",
       "      <td>American Heart Journal</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>custom_license</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aecbc613ebdab36753235197ffb4f35734b5ca63</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>Clinical and immunologic studies in identical ...</td>\n",
       "      <td>10.1016/0002-9343(73)90176-9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4579077.0</td>\n",
       "      <td>els-covid</td>\n",
       "      <td>Abstract Middle-aged female identical twins, o...</td>\n",
       "      <td>1973-08-31</td>\n",
       "      <td>Brunner, Carolyn M.; Horwitz, David A.; Shann,...</td>\n",
       "      <td>The American Journal of Medicine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>custom_license</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Elsevier</td>\n",
       "      <td>Epidemiology of community-acquired respiratory...</td>\n",
       "      <td>10.1016/0002-9343(85)90361-4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4014285.0</td>\n",
       "      <td>els-covid</td>\n",
       "      <td>Abstract Upper respiratory tract infections ar...</td>\n",
       "      <td>1985-06-28</td>\n",
       "      <td>Garibaldi, Richard A.</td>\n",
       "      <td>The American Journal of Medicine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>custom_license</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        sha  source_x  \\\n",
       "0                                       NaN  Elsevier   \n",
       "1                                       NaN  Elsevier   \n",
       "2                                       NaN  Elsevier   \n",
       "3  aecbc613ebdab36753235197ffb4f35734b5ca63  Elsevier   \n",
       "4                                       NaN  Elsevier   \n",
       "\n",
       "                                               title  \\\n",
       "0  Intrauterine virus infections and congenital h...   \n",
       "1                  Coronaviruses in Balkan nephritis   \n",
       "2  Cigarette smoking and coronary heart disease: ...   \n",
       "3  Clinical and immunologic studies in identical ...   \n",
       "4  Epidemiology of community-acquired respiratory...   \n",
       "\n",
       "                            doi pmcid  pubmed_id    license  \\\n",
       "0  10.1016/0002-8703(72)90077-4   NaN  4361535.0  els-covid   \n",
       "1  10.1016/0002-8703(80)90355-5   NaN  6243850.0  els-covid   \n",
       "2  10.1016/0002-8703(80)90356-7   NaN  7355701.0  els-covid   \n",
       "3  10.1016/0002-9343(73)90176-9   NaN  4579077.0  els-covid   \n",
       "4  10.1016/0002-9343(85)90361-4   NaN  4014285.0  els-covid   \n",
       "\n",
       "                                            abstract publish_time  \\\n",
       "0  Abstract The etiologic basis for the vast majo...   1972-12-31   \n",
       "1                                                NaN   1980-03-31   \n",
       "2                                                NaN   1980-03-31   \n",
       "3  Abstract Middle-aged female identical twins, o...   1973-08-31   \n",
       "4  Abstract Upper respiratory tract infections ar...   1985-06-28   \n",
       "\n",
       "                                             authors  \\\n",
       "0                                  Overall, James C.   \n",
       "1  Georgescu, Leonida; Diosi, Peter; Bu≈£iu, Ioan;...   \n",
       "2                                   Friedman, Gary D   \n",
       "3  Brunner, Carolyn M.; Horwitz, David A.; Shann,...   \n",
       "4                              Garibaldi, Richard A.   \n",
       "\n",
       "                            journal  Microsoft Academic Paper ID  \\\n",
       "0            American Heart Journal                          NaN   \n",
       "1            American Heart Journal                          NaN   \n",
       "2            American Heart Journal                          NaN   \n",
       "3  The American Journal of Medicine                          NaN   \n",
       "4  The American Journal of Medicine                          NaN   \n",
       "\n",
       "  WHO #Covidence  has_full_text  full_text_file  \n",
       "0            NaN          False  custom_license  \n",
       "1            NaN          False  custom_license  \n",
       "2            NaN          False  custom_license  \n",
       "3            NaN           True  custom_license  \n",
       "4            NaN          False  custom_license  "
      ]
     },
     "execution_count": 706,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Risk Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do we know about COVID-19 risk factors?\n",
    "The hypothesis is that if a paper's abstract has smok and pulm together, it presents papers on Smoking, pre-existing pulmonary diseases as risk factors. We started with a high recall approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_ind = []\n",
    "count_risk = []\n",
    "abst_risk = []\n",
    "abstract = list(new_data['abstract'])\n",
    "for i in abstract:\n",
    "    if (str(i).lower().find('smok') != -1 or str(i).lower().find('pulm') != -1 ):\n",
    "        abst_risk.append(i)\n",
    "        count_risk.append(i.lower().count('risk')/len(i.lower()))\n",
    "corona_df_risk_smoke = new_data[new_data['abstract'].isin(abst_risk)] \n",
    "corona_df_risk_smoke['count_risk'] = count_risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 688,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'doc_id', 'source', 'title', 'abstract', 'text_body',\n",
       "       'index', 'count_risk', 'sha', 'source_x', 'doi', 'pmcid', 'pubmed_id',\n",
       "       'license', 'publish_time', 'authors', 'journal',\n",
       "       'Microsoft Academic Paper ID', 'WHO #Covidence', 'has_full_text',\n",
       "       'full_text_file'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 688,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corona_df_risk_smoke.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using h_index as a factor for ranking the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 761,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h_index = pd.read_csv('/Users/u6066091/Downloads/scimagoj_2018.csv',sep = ';')\n",
    "# journal_h_index = list(corona_df_risk_covid[corona_df_risk_covid['journal'].isin(h_index['Title'])]['journal'].unique())\n",
    "df_smoke = corona_df_risk_smoke.join(h_index, lsuffix='journal', rsuffix='Title')\n",
    "df_smoke.columns = ['Unnamed: 0', 'doc_id', 'source', 'title', 'abstract', 'text_body',\n",
    "       'index', 'count_risk', 'sha', 'source_x', 'doi', 'pmcid', 'pubmed_id',\n",
    "       'license', 'publish_time', 'authors', 'journal',\n",
    "       'Microsoft Academic Paper ID', 'WHO #Covidence', 'has_full_text',\n",
    "       'full_text_file', 'Rank', 'Sourceid', 'Title', 'Type', 'Issn', 'SJR',\n",
    "       'SJR Best Quartile', 'H_index', 'Total Docs. (2018)',\n",
    "       'Total Docs. (3years)', 'Total Refs.', 'Total Cites (3years)',\n",
    "       'Citable Docs. (3years)', 'Cites / Doc. (2years)', 'Ref. / Doc.',\n",
    "       'Country', 'Publisher', 'Coverage', 'Categories']\n",
    "\n",
    "df_smoke = df_smoke[['doc_id', 'source', 'title', 'abstract', 'text_body',\n",
    "       'index', 'count_risk', 'sha', 'source_x', 'doi', 'pmcid', 'pubmed_id',\n",
    "       'license', 'publish_time', 'authors', 'journal',\n",
    "       'Microsoft Academic Paper ID', 'WHO #Covidence', 'has_full_text',\n",
    "       'full_text_file', 'Rank', 'Sourceid','Type', 'Issn', 'SJR',\n",
    "       'SJR Best Quartile', 'H_index', 'Total Docs. (2018)',\n",
    "       'Total Docs. (3years)', 'Total Refs.', 'Total Cites (3years)',\n",
    "       'Citable Docs. (3years)', 'Cites / Doc. (2years)', 'Ref. / Doc.',\n",
    "       'Country', 'Publisher', 'Coverage', 'Categories']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "risk_count = [str(i).lower().count('risk')/len(str(i).lower()) for i in df_smoke_sel['text_body']]\n",
    "covid_count = [str(i).lower().count('covi')*10/len(str(i).lower()) for i in df_smoke_sel['text_body']]\n",
    "smok_count = [str(i).lower().count('smok')/len(str(i).lower()) for i in df_smoke_sel['text_body']]\n",
    "pulm_count = [str(i).lower().count('pulm')/len(str(i).lower()) for i in df_smoke_sel['text_body']]\n",
    "\n",
    "normalized_count = np.sum([risk_count, covid_count,smok_count, pulm_count] , axis = 0)\n",
    "rank = np.array(normalized_count)*100 + np.mean(df_smoke_sel['H_index'])/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 772,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smoke['rank'] = rank\n",
    "df_smoke_sel = df_smoke.sort_values('rank', ascending = False).reset_index(drop = True).loc[0:40, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 775,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_smoke_sel.to_csv('.../Desktop/kaggle/output/corona_challenge/curated/keshav/smoke_sel.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphShow():\n",
    "    \"\"\"\"Create demo page\"\"\"\n",
    "    def __init__(self):\n",
    "        self.base = '''\n",
    "    <html>\n",
    "    <head>\n",
    "      <script type=\"text/javascript\" src=\"VIS/dist/vis.js\"></script>\n",
    "      <link href=\"VIS/dist/vis.css\" rel=\"stylesheet\" type=\"text/css\">\n",
    "      <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n",
    "    </head>\n",
    "    <body>\n",
    "    <div id=\"VIS_draw\"></div>\n",
    "    <script type=\"text/javascript\">\n",
    "      var nodes = data_nodes;\n",
    "      var edges = data_edges;\n",
    "      var container = document.getElementById(\"VIS_draw\");\n",
    "      var data = {\n",
    "        nodes: nodes,\n",
    "        edges: edges\n",
    "      };\n",
    "      var options = {\n",
    "          nodes: {\n",
    "              shape: 'circle',\n",
    "              size: 15,\n",
    "              font: {\n",
    "                  size: 15\n",
    "              }\n",
    "          },\n",
    "          edges: {\n",
    "              font: {\n",
    "                  size: 10,\n",
    "                  align: 'center'\n",
    "              },\n",
    "              color: 'red',\n",
    "              arrows: {\n",
    "                  to: {enabled: true, scaleFactor: 1.2}\n",
    "              },\n",
    "              smooth: {enabled: true}\n",
    "          },\n",
    "          physics: {\n",
    "              enabled: true\n",
    "          }\n",
    "      };\n",
    "      var network = new vis.Network(container, data, options);\n",
    "    </script>\n",
    "    </body>\n",
    "    </html>\n",
    "    '''\n",
    "    \n",
    "\n",
    "    def create_page(self, events):\n",
    "        \"\"\"Read data\"\"\"\n",
    "        nodes = []\n",
    "        for event in events:\n",
    "            nodes.append(event[0])\n",
    "            nodes.append(event[1])\n",
    "        node_dict = {node: index for index, node in enumerate(nodes)}\n",
    "\n",
    "        data_nodes = []\n",
    "        data_edges = []\n",
    "        for node, id in node_dict.items():\n",
    "            data = {}\n",
    "            data[\"group\"] = 'Event'\n",
    "            data[\"id\"] = id\n",
    "            data[\"label\"] = node\n",
    "            data_nodes.append(data)\n",
    "\n",
    "        for edge in events:\n",
    "            data = {}\n",
    "            data['from'] = node_dict.get(edge[0])\n",
    "            data['label'] = ''\n",
    "            data['to'] = node_dict.get(edge[1])\n",
    "            data_edges.append(data)\n",
    "\n",
    "        self.create_html(data_nodes, data_edges)\n",
    "        return\n",
    "\n",
    "    def create_html(self, data_nodes, data_edges):\n",
    "        \"\"\"Generate html file\"\"\"\n",
    "        f = open('graph_show.html', 'w+')\n",
    "        html = self.base.replace('data_nodes', str(data_nodes)).replace('data_edges', str(data_edges))\n",
    "        f.write(html)\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install spacy\n",
    "# !pip install scispacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class TextrankGraph:\n",
    "    '''textrank graph'''\n",
    "    def __init__(self):\n",
    "        self.graph = defaultdict(list)\n",
    "        self.d = 0.85 # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5 # convergence threshold\n",
    "        self.steps = 1000 # iteration steps\n",
    "\n",
    "    def addEdge(self, start, end, weight):\n",
    "        \"\"\"Add edge between node\"\"\"\n",
    "        self.graph[start].append((start, end, weight))\n",
    "        self.graph[end].append((end, start, weight))\n",
    "\n",
    "    def rank(self):\n",
    "        \"\"\"Rank all nodes\"\"\"\n",
    "        weight_deafault = 1.0 / (len(self.graph) or 1.0) # initialize weight\n",
    "        nodeweight_dict = defaultdict(float) # store weight of node\n",
    "        outsum_node_dict = defaultdict(float) # store wegiht of out nodes\n",
    "        for node, out_edge in self.graph.items(): # initilize nodes weight by edges\n",
    "            # node: was\n",
    "            # out_edge: [('was', 'prison', 1), ('was', 'wrong', 1), ('was', 'bad', 1)]\n",
    "            nodeweight_dict[node] = weight_deafault\n",
    "            outsum_node_dict[node] = sum((edge[2] for edge in out_edge), 0.0) # if no out edge, set weight 0\n",
    "        \n",
    "        sorted_keys = sorted(self.graph.keys()) # save node name as a list for iteration\n",
    "        step_dict = [0]\n",
    "        for step in range(1, self.steps):\n",
    "            for node in sorted_keys:\n",
    "                s = 0\n",
    "                # Node's weight calculation: \n",
    "                # (edge_weight/ node's number of out link)*node_weight[edge_node]\n",
    "                for e in self.graph[node]:\n",
    "                    s += e[2] / outsum_node_dict[e[1]] * nodeweight_dict[e[1]]\n",
    "                # Update calculation: (1-d) + d*s\n",
    "                nodeweight_dict[node] = (1 - self.d) + self.d * s\n",
    "            step_dict.append(sum(nodeweight_dict.values()))\n",
    "\n",
    "            if abs(step_dict[step] - step_dict[step - 1]) <= self.min_diff:\n",
    "                break\n",
    "\n",
    "        # min-max scale to make result in range to [0 - 1]\n",
    "        min_rank, max_rank = 0, 0 # initilize max and min wegiht value\n",
    "        for w in nodeweight_dict.values():\n",
    "            if w < min_rank:\n",
    "                min_rank = w\n",
    "            if w > max_rank:\n",
    "                max_rank = w\n",
    "\n",
    "        for n, w in nodeweight_dict.items():\n",
    "            nodeweight_dict[n] = (w - min_rank/10.0) / (max_rank - min_rank/10.0)\n",
    "\n",
    "        return nodeweight_dict\n",
    "\n",
    "\n",
    "class TextRank:\n",
    "    \"\"\"Extract keywords based on textrank graph algorithm\"\"\"\n",
    "    def __init__(self):\n",
    "        self.candi_pos = ['NOUN', 'PROPN', 'VERB'] # ÂêçËØçÔºå‰∏ìÊúâÂêçËØçÔºåÂä®ËØç\n",
    "        self.stop_pos = ['NUM', 'ADV'] # Êï∞Â≠óÔºàÊ≤°ÊúâÊó∂Èó¥ÂêçËØçÔºåÂ∞±Áî®Êï∞Â≠ó‰ª£Ë°®‰∫ÜÔºâÔºåÂâØËØç\n",
    "        self.span = 5\n",
    "\n",
    "    def extract_keywords(self, word_list, num_keywords):\n",
    "        g = TextrankGraph()\n",
    "        cm = defaultdict(int)\n",
    "        for i, word in enumerate(word_list): # word_list = [['previous', 'ADJ'], ['rumor', 'NOUN']]\n",
    "            if word[1] in self.candi_pos and len(word[0]) > 1: # word = ['previous', 'ADJ']\n",
    "                for j in range(i + 1, i + self.span):\n",
    "                    if j >= len(word_list):\n",
    "                        break\n",
    "                    if word_list[j][1] not in self.candi_pos or word_list[j][1] in self.stop_pos or len(word_list[j][0]) < 2:\n",
    "                        continue\n",
    "                    pair = tuple((word[0], word_list[j][0]))\n",
    "                    cm[(pair)] +=  1\n",
    "\n",
    "        # cm = {('was', 'prison'): 1, ('become', 'prison'): 1}\n",
    "        for terms, w in cm.items():\n",
    "            g.addEdge(terms[0], terms[1], w)\n",
    "        nodes_rank = g.rank()\n",
    "        nodes_rank = sorted(nodes_rank.items(), key=lambda asd:asd[1], reverse=True)\n",
    "\n",
    "        return nodes_rank[:num_keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import scispacy\n",
    "# import GraphShow\n",
    "# from textrank import TextRank\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "class NewsMining():\n",
    "    \"\"\"News Mining\"\"\"\n",
    "    def __init__(self):\n",
    "        self.textranker = TextRank()\n",
    "        self.ners = ['PERSON', 'ORG', 'GPE']\n",
    "        self.ner_dict = {\n",
    "            'PERSON': 'Person',  # People, including fictional\n",
    "            'ORG': 'Organization',  # Companies, agencies, institutions, etc.\n",
    "            'GPE': 'Location',  # Countries, cities, states.\n",
    "        }\n",
    "        # dependency markers for subjects\n",
    "        self.SUBJECTS = {\"nsubj\", \"nsubjpass\",\n",
    "                         \"csubj\", \"csubjpass\", \"agent\", \"expl\"}\n",
    "        # dependency markers for objects\n",
    "        self.OBJECTS = {\"dobj\", \"dative\", \"attr\", \"oprd\"}\n",
    "\n",
    "        self.graph_shower = GraphShow()\n",
    "\n",
    "    def clean_spaces(self, s):\n",
    "        s = s.replace('\\r', '')\n",
    "        s = s.replace('\\t', ' ')\n",
    "        s = s.replace('\\n', ' ')\n",
    "        return s\n",
    "\n",
    "    def remove_noisy(self, content):\n",
    "        \"\"\"Remove brackets\"\"\"\n",
    "        p1 = re.compile(r'Ôºà[^Ôºâ]*Ôºâ')\n",
    "        p2 = re.compile(r'\\([^\\)]*\\)')\n",
    "        return p2.sub('', p1.sub('', content))\n",
    "\n",
    "    def collect_ners(self, ents):\n",
    "        \"\"\"Collect token only with PERSON, ORG, GPE\"\"\"\n",
    "        collected_ners = []\n",
    "        for token in ents:\n",
    "            if token.label_ in self.ners:\n",
    "                collected_ners.append(token.text + '/' + token.label_)\n",
    "        return collected_ners\n",
    "\n",
    "    def conll_syntax(self, sent):\n",
    "        \"\"\"Convert one sentence to conll format.\"\"\"\n",
    "\n",
    "        tuples = list()\n",
    "        for word in sent:\n",
    "            if word.head is word:\n",
    "                head_idx = 0\n",
    "            else:\n",
    "                head_idx = word.head.i + 1\n",
    "            tuples.append([word.i + 1,  # Current word index, begin with 1\n",
    "                           word.text,  # Word\n",
    "                           word.lemma_,  # Lemma\n",
    "                           word.pos_,  # Coarse-grained tag\n",
    "                           word.tag_,  # Fine-grained tag\n",
    "                           '_',\n",
    "                           head_idx,  # Head of current  Index\n",
    "                           word.dep_,  # Relation\n",
    "                           '_', '_'])\n",
    "        return tuples\n",
    "\n",
    "    def syntax_parse(self, sent):\n",
    "        \"\"\"Convert one sentence to conll format.\"\"\"\n",
    "        tuples = list()\n",
    "        for word in sent:\n",
    "            if word.head is word:\n",
    "                head_idx = 0\n",
    "            else:\n",
    "                head_idx = word.head.i + 1\n",
    "            tuples.append([word.i + 1,  # Current word index, begin with 1\n",
    "                           word.text,  # Word\n",
    "                           word.pos_,  # Coarse-grained tag\n",
    "                           word.head,\n",
    "                           head_idx,  # Head of current  Index\n",
    "                           word.dep_,  # Relation\n",
    "                           ])\n",
    "        return tuples\n",
    "\n",
    "    def build_parse_chile_dict(self, sent, tuples):\n",
    "        child_dict_list = list()\n",
    "        for word in sent:\n",
    "            child_dict = dict()\n",
    "            for arc in tuples:\n",
    "                if arc[3] == word:\n",
    "                    if arc[-1] in child_dict:\n",
    "                        child_dict[arc[-1]].append(arc)\n",
    "                    else:\n",
    "                        child_dict[arc[-1]] = []\n",
    "                        child_dict[arc[-1]].append(arc)\n",
    "            child_dict_list.append([word, word.pos_, word.i, child_dict])\n",
    "        return child_dict_list\n",
    "\n",
    "    def complete_VOB(self, verb, child_dict_list):\n",
    "        '''Find VOB by SBV'''\n",
    "        for child in child_dict_list:\n",
    "            word = child[0]\n",
    "            # child_dict: {'dobj': [[7, 'startup', 'NOUN', buying, 5, 'dobj']], 'prep': [[8, 'for', 'ADP', buying, 5, 'prep']]}\n",
    "            child_dict = child[3]\n",
    "            if word == verb:\n",
    "                for object_type in self.OBJECTS:  # object_type: 'dobj'\n",
    "                    if object_type not in child_dict:\n",
    "                        continue\n",
    "                    # [7, 'startup', 'NOUN', buying, 5, 'dobj']\n",
    "                    vob = child_dict[object_type][0]\n",
    "                    obj = vob[1]  # 'startup'\n",
    "                    return obj\n",
    "        return ''\n",
    "\n",
    "    def extract_triples(self, sent):\n",
    "        svo = []\n",
    "        tuples = self.syntax_parse(sent)\n",
    "        child_dict_list = self.build_parse_chile_dict(sent, tuples)\n",
    "        for tuple in tuples:\n",
    "            rel = tuple[-1]\n",
    "            if rel in self.SUBJECTS:\n",
    "                sub_wd = tuple[1]\n",
    "                verb_wd = tuple[3]\n",
    "                obj = self.complete_VOB(verb_wd, child_dict_list)\n",
    "                subj = sub_wd\n",
    "                verb = verb_wd.text\n",
    "                if not obj:\n",
    "                    svo.append([subj, verb])\n",
    "                else:\n",
    "                    svo.append([subj, verb+' '+obj])\n",
    "        return svo\n",
    "\n",
    "    def extract_keywords(self, words_postags):\n",
    "        return self.textranker.extract_keywords(words_postags, 10)\n",
    "\n",
    "    def collect_coexist(self, ner_sents, ners):\n",
    "        \"\"\"Construct NER co-occurrence matrices\"\"\"\n",
    "        co_list = []\n",
    "        for words in ner_sents:\n",
    "            co_ners = set(ners).intersection(set(words))\n",
    "            co_info = self.combination(list(co_ners))\n",
    "            co_list += co_info\n",
    "        if not co_list:\n",
    "            return []\n",
    "        return {i[0]: i[1] for i in Counter(co_list).most_common()}\n",
    "\n",
    "    def combination(self, a):\n",
    "        '''list all combination'''\n",
    "        combines = []\n",
    "        if len(a) == 0:\n",
    "            return []\n",
    "        for i in a:\n",
    "            for j in a:\n",
    "                if i == j:\n",
    "                    continue\n",
    "                combines.append('@'.join([i, j]))\n",
    "        return combines\n",
    "\n",
    "    def main(self, content):\n",
    "        '''Main function'''\n",
    "        if not content:\n",
    "            return []\n",
    "\n",
    "        words_postags = []  # token and its POS tag\n",
    "        ner_sents = []      # store sentences which contain NER entity\n",
    "        ners = []           # store all NER entity from whole article\n",
    "        triples = []        # store subject verb object\n",
    "        events = []         # store events\n",
    "\n",
    "        # 01 remove linebreaks and brackets\n",
    "        content = self.remove_noisy(content)\n",
    "        content = self.clean_spaces(content)\n",
    "\n",
    "        # 02 split to sentences\n",
    "        doc = nlp(content)\n",
    "\n",
    "        for i, sent in enumerate(doc.sents):\n",
    "            words_postags = [[token.text, token.pos_] for token in sent]\n",
    "            words = [token.text for token in sent]\n",
    "            postags = [token.pos_ for token in sent]\n",
    "            ents = nlp(sent.text).ents  # NER detection\n",
    "            collected_ners = self.collect_ners(ents)\n",
    "\n",
    "            if collected_ners:  # only extract triples when the sentence contains 'PERSON', 'ORG', 'GPE'\n",
    "                triple = self.extract_triples(sent)\n",
    "                if not triple:\n",
    "                    continue\n",
    "                triples += triple\n",
    "                ners += collected_ners\n",
    "                ner_sents.append(\n",
    "                    [token.text + '/' + token.label_ for token in sent.ents])\n",
    "\n",
    "        # 03 get keywords\n",
    "        keywords = [i[0] for i in self.extract_keywords(words_postags)]\n",
    "        for keyword in keywords:\n",
    "            name = keyword\n",
    "            cate = 'keyword'\n",
    "            events.append([name, cate])\n",
    "\n",
    "        # 04 add triples to event only the word in keyword\n",
    "        for t in triples:\n",
    "            if (t[0] in keywords or t[1] in keywords) and len(t[0]) > 1 and len(t[1]) > 1:\n",
    "                events.append([t[0], t[1]])\n",
    "\n",
    "        # 05 get word frequency and add to events\n",
    "        word_dict = [i for i in Counter([i[0] for i in words_postags if i[1] in [\n",
    "                                        'NOUN', 'PROPN', 'VERB'] and len(i[0]) > 1]).most_common()][:10]\n",
    "        for wd in word_dict:\n",
    "            name = wd[0]\n",
    "            cate = 'frequency'\n",
    "            events.append([name, cate])\n",
    "\n",
    "        # 06 get NER from whole article\n",
    "        ner_dict = {i[0]: i[1] for i in Counter(ners).most_common(20)}\n",
    "        for ner in ner_dict:\n",
    "            name = ner.split('/')[0]  # Jessica Miller\n",
    "            cate = self.ner_dict[ner.split('/')[1]]  # PERSON\n",
    "            events.append([name, cate])\n",
    "\n",
    "        # 07 get all NER entity co-occurrence information\n",
    "        # here ner_dict is from above 06\n",
    "        co_dict = self.collect_coexist(ner_sents, list(ner_dict.keys()))\n",
    "        co_events = [[i.split('@')[0].split(\n",
    "            '/')[0], i.split('@')[1].split('/')[0]] for i in co_dict]\n",
    "        events += co_events\n",
    "\n",
    "        # 08 show event graph\n",
    "        self.graph_shower.create_page(events)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "import spacy\n",
    "# from graph_show import GraphShow\n",
    "# from textrank import TextRank\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "class NewsMining():\n",
    "    \"\"\"News Mining\"\"\"\n",
    "    def __init__(self):\n",
    "        self.textranker = TextRank()\n",
    "        self.ners = ['PERSON', 'ORG', 'GPE']\n",
    "        self.ner_dict = {\n",
    "            'PERSON': 'Person',  # People, including fictional\n",
    "            'ORG': 'Organization',  # Companies, agencies, institutions, etc.\n",
    "            'GPE': 'Location',  # Countries, cities, states.\n",
    "        }\n",
    "        # dependency markers for subjects\n",
    "        self.SUBJECTS = {\"nsubj\", \"nsubjpass\",\n",
    "                         \"csubj\", \"csubjpass\", \"agent\", \"expl\"}\n",
    "        # dependency markers for objects\n",
    "        self.OBJECTS = {\"dobj\", \"dative\", \"attr\", \"oprd\"}\n",
    "\n",
    "        self.graph_shower = GraphShow()\n",
    "\n",
    "    def clean_spaces(self, s):\n",
    "        s = s.replace('\\r', '')\n",
    "        s = s.replace('\\t', ' ')\n",
    "        s = s.replace('\\n', ' ')\n",
    "        return s\n",
    "\n",
    "    def remove_noisy(self, content):\n",
    "        \"\"\"Remove brackets\"\"\"\n",
    "        p1 = re.compile(r'Ôºà[^Ôºâ]*Ôºâ')\n",
    "        p2 = re.compile(r'\\([^\\)]*\\)')\n",
    "        return p2.sub('', p1.sub('', content))\n",
    "\n",
    "    def collect_ners(self, ents):\n",
    "        \"\"\"Collect token only with PERSON, ORG, GPE\"\"\"\n",
    "        collected_ners = []\n",
    "        for token in ents:\n",
    "            if token.label_ in self.ners:\n",
    "                collected_ners.append(token.text + '/' + token.label_)\n",
    "        return collected_ners\n",
    "\n",
    "    def conll_syntax(self, sent):\n",
    "        \"\"\"Convert one sentence to conll format.\"\"\"\n",
    "\n",
    "        tuples = list()\n",
    "        for word in sent:\n",
    "            if word.head is word:\n",
    "                head_idx = 0\n",
    "            else:\n",
    "                head_idx = word.head.i + 1\n",
    "            tuples.append([word.i + 1,  # Current word index, begin with 1\n",
    "                           word.text,  # Word\n",
    "                           word.lemma_,  # Lemma\n",
    "                           word.pos_,  # Coarse-grained tag\n",
    "                           word.tag_,  # Fine-grained tag\n",
    "                           '_',\n",
    "                           head_idx,  # Head of current  Index\n",
    "                           word.dep_,  # Relation\n",
    "                           '_', '_'])\n",
    "        return tuples\n",
    "\n",
    "    def syntax_parse(self, sent):\n",
    "        \"\"\"Convert one sentence to conll format.\"\"\"\n",
    "        tuples = list()\n",
    "        for word in sent:\n",
    "            if word.head is word:\n",
    "                head_idx = 0\n",
    "            else:\n",
    "                head_idx = word.head.i + 1\n",
    "            tuples.append([word.i + 1,  # Current word index, begin with 1\n",
    "                           word.text,  # Word\n",
    "                           word.pos_,  # Coarse-grained tag\n",
    "                           word.head,\n",
    "                           head_idx,  # Head of current  Index\n",
    "                           word.dep_,  # Relation\n",
    "                           ])\n",
    "        return tuples\n",
    "\n",
    "    def build_parse_chile_dict(self, sent, tuples):\n",
    "        child_dict_list = list()\n",
    "        for word in sent:\n",
    "            child_dict = dict()\n",
    "            for arc in tuples:\n",
    "                if arc[3] == word:\n",
    "                    if arc[-1] in child_dict:\n",
    "                        child_dict[arc[-1]].append(arc)\n",
    "                    else:\n",
    "                        child_dict[arc[-1]] = []\n",
    "                        child_dict[arc[-1]].append(arc)\n",
    "            child_dict_list.append([word, word.pos_, word.i, child_dict])\n",
    "        return child_dict_list\n",
    "\n",
    "    def complete_VOB(self, verb, child_dict_list):\n",
    "        '''Find VOB by SBV'''\n",
    "        for child in child_dict_list:\n",
    "            word = child[0]\n",
    "            # child_dict: {'dobj': [[7, 'startup', 'NOUN', buying, 5, 'dobj']], 'prep': [[8, 'for', 'ADP', buying, 5, 'prep']]}\n",
    "            child_dict = child[3]\n",
    "            if word == verb:\n",
    "                for object_type in self.OBJECTS:  # object_type: 'dobj'\n",
    "                    if object_type not in child_dict:\n",
    "                        continue\n",
    "                    # [7, 'startup', 'NOUN', buying, 5, 'dobj']\n",
    "                    vob = child_dict[object_type][0]\n",
    "                    obj = vob[1]  # 'startup'\n",
    "                    return obj\n",
    "        return ''\n",
    "\n",
    "    def extract_triples(self, sent):\n",
    "        svo = []\n",
    "        tuples = self.syntax_parse(sent)\n",
    "        child_dict_list = self.build_parse_chile_dict(sent, tuples)\n",
    "        for tuple in tuples:\n",
    "            rel = tuple[-1]\n",
    "            if rel in self.SUBJECTS:\n",
    "                sub_wd = tuple[1]\n",
    "                verb_wd = tuple[3]\n",
    "                obj = self.complete_VOB(verb_wd, child_dict_list)\n",
    "                subj = sub_wd\n",
    "                verb = verb_wd.text\n",
    "                if not obj:\n",
    "                    svo.append([subj, verb])\n",
    "                else:\n",
    "                    svo.append([subj, verb+' '+obj])\n",
    "        return svo\n",
    "\n",
    "    def extract_keywords(self, words_postags):\n",
    "        return self.textranker.extract_keywords(words_postags, 10)\n",
    "\n",
    "    def collect_coexist(self, ner_sents, ners):\n",
    "        \"\"\"Construct NER co-occurrence matrices\"\"\"\n",
    "        co_list = []\n",
    "        for words in ner_sents:\n",
    "            co_ners = set(ners).intersection(set(words))\n",
    "            co_info = self.combination(list(co_ners))\n",
    "            co_list += co_info\n",
    "        if not co_list:\n",
    "            return []\n",
    "        return {i[0]: i[1] for i in Counter(co_list).most_common()}\n",
    "\n",
    "    def combination(self, a):\n",
    "        '''list all combination'''\n",
    "        combines = []\n",
    "        if len(a) == 0:\n",
    "            return []\n",
    "        for i in a:\n",
    "            for j in a:\n",
    "                if i == j:\n",
    "                    continue\n",
    "                combines.append('@'.join([i, j]))\n",
    "        return combines\n",
    "\n",
    "    def main(self, content):\n",
    "        '''Main function'''\n",
    "        if not content:\n",
    "            return []\n",
    "\n",
    "        words_postags = []  # token and its POS tag\n",
    "        ner_sents = []      # store sentences which contain NER entity\n",
    "        ners = []           # store all NER entity from whole article\n",
    "        triples = []        # store subject verb object\n",
    "        events = []         # store events\n",
    "\n",
    "        # 01 remove linebreaks and brackets\n",
    "        content = self.remove_noisy(content)\n",
    "        content = self.clean_spaces(content)\n",
    "\n",
    "        # 02 split to sentences\n",
    "        doc = nlp(content)\n",
    "\n",
    "        for i, sent in enumerate(doc.sents):\n",
    "            words_postags = [[token.text, token.pos_] for token in sent]\n",
    "            words = [token.text for token in sent]\n",
    "            postags = [token.pos_ for token in sent]\n",
    "            ents = nlp(sent.text).ents  # NER detection\n",
    "            collected_ners = self.collect_ners(ents)\n",
    "\n",
    "            if collected_ners:  # only extract triples when the sentence contains 'PERSON', 'ORG', 'GPE'\n",
    "                triple = self.extract_triples(sent)\n",
    "                if not triple:\n",
    "                    continue\n",
    "                triples += triple\n",
    "                ners += collected_ners\n",
    "                ner_sents.append(\n",
    "                    [token.text + '/' + token.label_ for token in sent.ents])\n",
    "\n",
    "        # 03 get keywords\n",
    "        keywords = [i[0] for i in self.extract_keywords(words_postags)]\n",
    "        for keyword in keywords:\n",
    "            name = keyword\n",
    "            cate = 'keyword'\n",
    "            events.append([name, cate])\n",
    "\n",
    "        # 04 add triples to event only the word in keyword\n",
    "        for t in triples:\n",
    "            if (t[0] in keywords or t[1] in keywords) and len(t[0]) > 1 and len(t[1]) > 1:\n",
    "                events.append([t[0], t[1]])\n",
    "\n",
    "        # 05 get word frequency and add to events\n",
    "        word_dict = [i for i in Counter([i[0] for i in words_postags if i[1] in [\n",
    "                                        'NOUN', 'PROPN', 'VERB'] and len(i[0]) > 1]).most_common()][:10]\n",
    "        for wd in word_dict:\n",
    "            name = wd[0]\n",
    "            cate = 'frequency'\n",
    "            events.append([name, cate])\n",
    "\n",
    "        # 06 get NER from whole article\n",
    "        ner_dict = {i[0]: i[1] for i in Counter(ners).most_common(20)}\n",
    "        for ner in ner_dict:\n",
    "            name = ner.split('/')[0]  # Jessica Miller\n",
    "            cate = self.ner_dict[ner.split('/')[1]]  # PERSON\n",
    "            events.append([name, cate])\n",
    "\n",
    "        # 07 get all NER entity co-occurrence information\n",
    "        # here ner_dict is from above 06\n",
    "        co_dict = self.collect_coexist(ner_sents, list(ner_dict.keys()))\n",
    "        co_events = [[i.split('@')[0].split(\n",
    "            '/')[0], i.split('@')[1].split('/')[0]] for i in co_dict]\n",
    "        events += co_events\n",
    "\n",
    "        # 08 show event graph\n",
    "        self.graph_shower.create_page(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphShow():\n",
    "    \"\"\"\"Create demo page\"\"\"\n",
    "    def __init__(self):\n",
    "        self.base = '''\n",
    "    <html>\n",
    "    <head>\n",
    "      <script type=\"text/javascript\" src=\"VIS/dist/vis.js\"></script>\n",
    "      <link href=\"VIS/dist/vis.css\" rel=\"stylesheet\" type=\"text/css\">\n",
    "      <meta http-equiv=\"Content-Type\" content=\"text/html; charset=utf-8\" />\n",
    "    </head>\n",
    "    <body>\n",
    "    <div id=\"VIS_draw\"></div>\n",
    "    <script type=\"text/javascript\">\n",
    "      var nodes = data_nodes;\n",
    "      var edges = data_edges;\n",
    "      var container = document.getElementById(\"VIS_draw\");\n",
    "      var data = {\n",
    "        nodes: nodes,\n",
    "        edges: edges\n",
    "      };\n",
    "      var options = {\n",
    "          nodes: {\n",
    "              shape: 'circle',\n",
    "              size: 15,\n",
    "              font: {\n",
    "                  size: 15\n",
    "              }\n",
    "          },\n",
    "          edges: {\n",
    "              font: {\n",
    "                  size: 10,\n",
    "                  align: 'center'\n",
    "              },\n",
    "              color: 'red',\n",
    "              arrows: {\n",
    "                  to: {enabled: true, scaleFactor: 1.2}\n",
    "              },\n",
    "              smooth: {enabled: true}\n",
    "          },\n",
    "          physics: {\n",
    "              enabled: true\n",
    "          }\n",
    "      };\n",
    "      var network = new vis.Network(container, data, options);\n",
    "    </script>\n",
    "    </body>\n",
    "    </html>\n",
    "    '''\n",
    "    \n",
    "\n",
    "    def create_page(self, events):\n",
    "        \"\"\"Read data\"\"\"\n",
    "        nodes = []\n",
    "        for event in events:\n",
    "            nodes.append(event[0])\n",
    "            nodes.append(event[1])\n",
    "        node_dict = {node: index for index, node in enumerate(nodes)}\n",
    "\n",
    "        data_nodes = []\n",
    "        data_edges = []\n",
    "        for node, id in node_dict.items():\n",
    "            data = {}\n",
    "            data[\"group\"] = 'Event'\n",
    "            data[\"id\"] = id\n",
    "            data[\"label\"] = node\n",
    "            data_nodes.append(data)\n",
    "\n",
    "        for edge in events:\n",
    "            data = {}\n",
    "            data['from'] = node_dict.get(edge[0])\n",
    "            data['label'] = ''\n",
    "            data['to'] = node_dict.get(edge[1])\n",
    "            data_edges.append(data)\n",
    "\n",
    "        self.create_html(data_nodes, data_edges)\n",
    "        return\n",
    "\n",
    "    def create_html(self, data_nodes, data_edges):\n",
    "        \"\"\"Generate html file\"\"\"\n",
    "        f = open('graph_show.html', 'w+')\n",
    "        html = self.base.replace('data_nodes', str(data_nodes)).replace('data_edges', str(data_edges))\n",
    "        f.write(html)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = corona_df_risk_covid.at[11821,'text_body' ]\n",
    "Miner = NewsMining()\n",
    "Miner.main(content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
