{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are a team of three: one machine learning/NLP scientist, one front-end senior software engineer and one co-op data science intern/engineer. We all work in the same place, Thomson Reuters, and that’s how we know each other.  \n",
    "\n",
    "### Task: \n",
    "What do we know about COVID-19 risk factors? \n",
    "\n",
    "### How we approached the challenge: \n",
    "Our platform was built based on the following principles: \n",
    "- Empower users (health researchers) to conduct literature survey efficiently. \n",
    "- Adaptable to future needs and challenges of health researchers.  \n",
    "- Modularized to have the capability of being improved and polished in a short time and in parallel.  \n",
    "\n",
    "### Our hypotheses/why? \n",
    "\n",
    "AI can benefit researchers by extracting and visualizing information in the most efficient and relevant manner at scale.  In the absence of expert feedback and annotations, we have built/provided a platform that extracts and ranks relevant info with potential for improvement in the future by taking advantage of expert annotation.\n",
    "\n",
    "### How we solve it/why? \n",
    "\n",
    "The project was done in two slightly different approaches.  \n",
    "The first approach focused on developing an end to end pipeline to address the first subtask which was: \n",
    "- Data on potential risks factors \n",
    "- Smoking, pre-existing pulmonary disease \n",
    "- Co-infections (determine whether co-existing respiratory/viral infections make the virus more transmissible or virulent) and other co-morbidities \n",
    "- Neonates and pregnant women \n",
    "- Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences. \n",
    "\n",
    "We have developed a web app to visually accompany the analysis carried out for this task. \n",
    "The second approach was focused on question information retrieval but isn’t accompanied by with a web-app yet due to time constraint. \n",
    "\n",
    "### First approach had the following recipe: \n",
    "The platform is designed to visualize snippets of relevant topics through the following process: \n",
    "- Merge different sources of the data\n",
    "- Extract all COVID 19 related papers \n",
    "- Expand and process the list of keywords\n",
    "- Find excerpts of papers that include the keywords \n",
    "- Rank the excerpts\n",
    "- Create an extractive summary for each paper\n",
    "- Serve it to the web-app for visualization\n",
    "\n",
    "### Second approach had the following recipe: \n",
    "The platform is designed to rank snippets of relevant topics through the following process: \n",
    "\n",
    "- Merge different sources of the data\n",
    "- Extract all COVID 19 related paper s\n",
    "- Convert a question into set of keywords (The questions are extracted from the [medical dictionary](https://docs.google.com/spreadsheets/d/1NoiAFJoydk3zuc-G0qqROarkhaGpfgbQhTVYhbYtLCM/edit#gid=0) shared with Kaggle participants.\n",
    "- Expand the list of keywords\n",
    "- Find excerpts of papers that include the keywords \n",
    "- Rank the excerpts\n",
    "- Create an extractive summary for each paper \n",
    "\n",
    "### Pros and Cons of our approach and platform: \n",
    "\n",
    "#### Pros: \n",
    "\n",
    "- The code base is highly modular and simple  \n",
    "- The code is very well documented \n",
    "- The visualization resonates with researchers. We interviewed an expert in epidemiology, since that is our target user. He liked all aspects of it including the extractive summaries of the papers. He also noted that using this platform could reduce the time taken for a literature survey, which normally takes 3 to 4 months, to less than a month.  \n",
    "- The platform is taking a high recall approach to include everything \n",
    "- With a little bit of effort, we can convert this platform to an expert annotation platform where experts can interactively click on irrelevant sentences in the snippets and convert a completely unsupervised approach to a supervised learning task. \n",
    "\n",
    "#### Cons: \n",
    "- Keyword search is not the most efficient search. Word embedding is known to be a better approach for text analysis but there wasn’t enough time to implement and evaluate that. \n",
    "- Our ranking follows a simple approach, where an expert should decide on the metric for ranking the importance of a snippet. \n",
    "- The second approach is not coming with a visualization. \n",
    "- There may be more risk factors than what we currently searched for and an algorithm should extract the unknown risks too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import json\n",
    "import string\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "root = '../../kaggle_data/'\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "stop_words.extend(['within', 'what', 'how', 'eg', 'ie'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Parsing and Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `PaperLoader` class will load all papers for the challenge and provide an interface for us to obtain `DataFrames` to work with. The focus will be on:\n",
    "- Obtaining Paper title, Abstract, Body\n",
    "    - The text body is filtered to remove sections containing lots of citations and hyperlinks\n",
    "- Obtaining Authors, Journal of Publication, Publication Date and Publication Date\n",
    "- Obtaining journal ratings(H index) to potentially sort paper based on journal quality\n",
    "    - For the journal ratings, we use a list we obtained from **INSERT LINK HERE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperLoader():\n",
    "    \"\"\"\n",
    "    Loads, parses and merges metadata for papers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, no_bib=True):\n",
    "        \"\"\"\n",
    "        Initializes PaperLoader class to read all .json files from root_directory\n",
    "            \n",
    "            no_bib: if true, clean noisy sections with bibliographies\n",
    "            root_dir: root directory for papers\n",
    "        \"\"\"\n",
    "        self.ROOT_DIR = root_dir\n",
    "        self.JSON_FILES = glob.glob(f'{root}/**/*.json', recursive=True)\n",
    "        self.PAPERS_COLUMN = {\n",
    "            \"doc_id\": [None],\n",
    "            \"title\": [None],\n",
    "            \"abstract\": [None],\n",
    "            \"text_body\": [None]\n",
    "        }\n",
    "        self.PAPERS_DF = None\n",
    "        self.NO_BIB = no_bib\n",
    "\n",
    "    \n",
    "    def __clean_bib(self, body_text, thres):\n",
    "        \"\"\"\n",
    "        Removes sections with more than 5 URL/DOI/HTTP instances\n",
    "            \n",
    "            body_text: array of dictionaries for text_body\n",
    "            thres: number of hyperlinks tolerated before removal \n",
    "        \"\"\"\n",
    "        # Sometimes, the text body has duplicate sections consecutively.\n",
    "        merged_body = []\n",
    "        for segment in body_text:\n",
    "            # We will combine these duplicate sections\n",
    "            if len(merged_body) > 0:\n",
    "                if merged_body[-1]['section'] == segment['section']:\n",
    "                    merged_body[-1]['text'] += '\\n' + segment['text']\n",
    "                    continue\n",
    "            merged_body.append(segment)\n",
    "\n",
    "        merged_body = [\n",
    "            segment for segment in merged_body\n",
    "            if len(re.findall(\"(http|doi|www)\", segment['text'])) <= thres\n",
    "        ]\n",
    "        return merged_body\n",
    "\n",
    "\n",
    "    def create_paper_df(self):\n",
    "        \"\"\"\n",
    "        Creates a Pandas DataFrame from all json files in root_directory\n",
    "        Each json file represents a paper. \n",
    "        Features extracted are: doc_id, title, abstract, text_body\n",
    "        \"\"\"\n",
    "        df_list = []\n",
    "        \n",
    "        for i in tqdm(range(len(self.JSON_FILES))):\n",
    "            file_name = self.JSON_FILES[i]\n",
    "            \n",
    "            #Initialize row for returned df. Each row represents a paper\n",
    "            row = {x: None for x in self.PAPERS_COLUMN}\n",
    "\n",
    "            with open(file_name) as json_data:\n",
    "                data = json.load(json_data)\n",
    "\n",
    "                row['doc_id'] = data['paper_id']\n",
    "                row['title'] = data['metadata']['title']\n",
    "                \n",
    "                # If title is empty, we skip the paper\n",
    "                if len(row['title']) <= 2:\n",
    "                    continue\n",
    "\n",
    "                # If a paper does not have an abstract of a body, we will skip it\n",
    "                if ('abstract' not in data or 'body_text' not in data):\n",
    "                    continue\n",
    "                else:\n",
    "                    # Now need all of the abstract. Put it all in\n",
    "                    # a list then use str.join() \n",
    "                    abstract_list = [abst['text'] for abst in data['abstract']]\n",
    "                    abstract = \"\\n \".join(abstract_list)\n",
    "\n",
    "                # Skip the paper if abstract is empty\n",
    "                if len(abstract) <= 2:\n",
    "                    continue\n",
    "\n",
    "                row['abstract'] = abstract\n",
    "\n",
    "                # And lastly the body of the text.\n",
    "                # These clauses check if the user wants to clean up references\n",
    "                if self.NO_BIB:\n",
    "                    body_list = self.__clean_bib(data['body_text'], 4)\n",
    "                else:\n",
    "                    body_list = [bt for bt in data['body_text']]\n",
    "\n",
    "                row['text_body'] = body_list\n",
    "\n",
    "                df_list.append(row)\n",
    "        # create final dataframe\n",
    "        self.PAPERS_DF = pd.DataFrame(df_list)\n",
    "\n",
    "\n",
    "    def merge_metadata(self, metadata = 'metadata.csv'):\n",
    "        \"\"\"\n",
    "            Joins paper information with information on journal for paper,\n",
    "            authors, doi and published date  \n",
    "                metadata: path to csv file containing metadata\n",
    "        \"\"\"\n",
    "        metadata_df = pd.read_csv(self.ROOT_DIR + metadata)\n",
    "        metadata_df = metadata_df.loc[:, \n",
    "                          ['sha', 'publish_time', 'authors', 'journal', 'doi']]\n",
    "        self.PAPERS_DF = self.PAPERS_DF.merge(metadata_df,\n",
    "                                              left_on='doc_id',\n",
    "                                              right_on='sha',\n",
    "                                              how='inner')\n",
    "\n",
    "    def merge_journals(self):\n",
    "        \"\"\"\n",
    "        Joins paper information with information on journal ratings\n",
    "        Important column: H_Index\n",
    "        \"\"\"\n",
    "        journal_df = pd.read_csv(root + 'scimagoj_2018.csv', sep=';')\n",
    "        papers_ratings_df = self.PAPERS_DF.merge(\n",
    "            journal_df.loc[:, ['Title', 'H index']],\n",
    "            left_on='journal',\n",
    "            right_on='Title',\n",
    "            how='left')\n",
    "        papers_ratings_df = papers_ratings_df.drop(\n",
    "            ['sha', 'Title'], axis=1).reset_index(drop=True)\n",
    "        self.PAPERS_DF = papers_ratings_df\n",
    "\n",
    "    def get_df(self):\n",
    "        \"\"\"\n",
    "        Returns processed dataframe\n",
    "        \"\"\"\n",
    "        self.PAPERS_DF = self.PAPERS_DF.dropna(\n",
    "            subset=['abstract', 'text_body'])\n",
    "        return self.PAPERS_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now parse the papers from our data(root) directory and store them in `papers_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8244935d43f449aab7f520725211637",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=59311.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "paper_loader = PaperLoader(root)\n",
    "paper_loader.create_paper_df()\n",
    "paper_loader.merge_metadata()\n",
    "paper_loader.merge_journals()\n",
    "papers_df = paper_loader.get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>text_body</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>doi</th>\n",
       "      <th>H index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>306ef95a3a91e13a93bcc37fb2c509b67c0b5640</td>\n",
       "      <td>A Novel Approach for a Novel Pathogen: using a...</td>\n",
       "      <td>Thousands of people in the United States have ...</td>\n",
       "      <td>[{'text': 'The 2019 novel coronavirus (SARS-Co...</td>\n",
       "      <td>2020-03-12</td>\n",
       "      <td>Bryson-Cahn, Chloe; Duchin, Jeffrey; Makarewic...</td>\n",
       "      <td>Clin Infect Dis</td>\n",
       "      <td>10.1093/cid/ciaa256</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6599ebbef3d868afac9daa4f80fa075675cf03bc</td>\n",
       "      <td>International aviation emissions to 2025: Can ...</td>\n",
       "      <td>International aviation is growing rapidly, res...</td>\n",
       "      <td>[{'text': 'Sixty years ago, civil aviation was...</td>\n",
       "      <td>2009-01-31</td>\n",
       "      <td>Macintosh, Andrew; Wallace, Lailey</td>\n",
       "      <td>Energy Policy</td>\n",
       "      <td>10.1016/j.enpol.2008.08.029</td>\n",
       "      <td>178.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     doc_id  \\\n",
       "0  306ef95a3a91e13a93bcc37fb2c509b67c0b5640   \n",
       "1  6599ebbef3d868afac9daa4f80fa075675cf03bc   \n",
       "\n",
       "                                               title  \\\n",
       "0  A Novel Approach for a Novel Pathogen: using a...   \n",
       "1  International aviation emissions to 2025: Can ...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Thousands of people in the United States have ...   \n",
       "1  International aviation is growing rapidly, res...   \n",
       "\n",
       "                                           text_body publish_time  \\\n",
       "0  [{'text': 'The 2019 novel coronavirus (SARS-Co...   2020-03-12   \n",
       "1  [{'text': 'Sixty years ago, civil aviation was...   2009-01-31   \n",
       "\n",
       "                                             authors          journal  \\\n",
       "0  Bryson-Cahn, Chloe; Duchin, Jeffrey; Makarewic...  Clin Infect Dis   \n",
       "1                 Macintosh, Andrew; Wallace, Lailey    Energy Policy   \n",
       "\n",
       "                           doi  H index  \n",
       "0          10.1093/cid/ciaa256      NaN  \n",
       "1  10.1016/j.enpol.2008.08.029    178.0  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25312, 9)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering for covid-19 related papers released after 2019\n",
    "There is a lot of noise in this dataset due to information about other strains of coronavirus so we will select only the papers that are related to Covid-19. \n",
    "\n",
    "While the older papers may contain some important insight on the variance among the  different strains of coronavirus, for our purposes, we will only be looking at papers published on 2019 or later because that is when Covid-19 was first discovered in humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of keywords for covid-19\n",
    "cov_list = [\n",
    "    'novel coronavi',\n",
    "    'covid',\n",
    "    'cov_2',\n",
    "    'cord-19',\n",
    "    'cord 19',\n",
    "    '2019-nCoV',\n",
    "    '2019 ncov',\n",
    "    '2019 cov',\n",
    "    'wuhan coronavi',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `RelevantFilter` class will filter the dataframe from `PaperLoader` and filter for covid-19 papers published on 2019 or later. \n",
    "We will need to supply a list of covid-related keywords to filter from to the `constructor`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelevantFilter():\n",
    "    \n",
    "    def __init__(self, keywords, year='2019'):\n",
    "        \"\"\"\n",
    "        constructor for RelevantFilter\n",
    "            keywords: keywords to filter for\n",
    "            year: papers written before this year will be discarded\n",
    "        \"\"\"\n",
    "        self.KEYWORDS = keywords\n",
    "        self.YEAR = year\n",
    "\n",
    "    def extract_recent(self, df):\n",
    "        \"\"\"\n",
    "        extracts documents published on or after self.YEAR\n",
    "        \"\"\"\n",
    "        return df[df['publish_time'] >= self.YEAR]\n",
    "\n",
    "    def filter_papers(self, df):\n",
    "        \"\"\"\n",
    "        Filters for papers whose title have mention of \n",
    "        any of the terms in self.KEYWORDS\n",
    "        \"\"\"\n",
    "        pattern = re.compile('(' + \"|\".join(self.KEYWORDS) + ')',\n",
    "                                 re.IGNORECASE)\n",
    "        # We will filter for rows with one or more matches \n",
    "        # for title and covid keywords\n",
    "        df = df[df['title'].apply(lambda x: \n",
    "                                  len(pattern.findall(x)) >= 1\n",
    "                                  if x else False)]\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will filter through `papers_df` to get only covid-19 related papers in `covid_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_filter = RelevantFilter(cov_list, '2019')\n",
    "covid_df = covid_filter.filter_papers(papers_df)\n",
    "covid_df = covid_filter.extract_recent(covid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(929, 9)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>text_body</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>doi</th>\n",
       "      <th>H index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>306ef95a3a91e13a93bcc37fb2c509b67c0b5640</td>\n",
       "      <td>A Novel Approach for a Novel Pathogen: using a...</td>\n",
       "      <td>Thousands of people in the United States have ...</td>\n",
       "      <td>[{'text': 'The 2019 novel coronavirus (SARS-Co...</td>\n",
       "      <td>2020-03-12</td>\n",
       "      <td>Bryson-Cahn, Chloe; Duchin, Jeffrey; Makarewic...</td>\n",
       "      <td>Clin Infect Dis</td>\n",
       "      <td>10.1093/cid/ciaa256</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     doc_id  \\\n",
       "0  306ef95a3a91e13a93bcc37fb2c509b67c0b5640   \n",
       "\n",
       "                                               title  \\\n",
       "0  A Novel Approach for a Novel Pathogen: using a...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Thousands of people in the United States have ...   \n",
       "\n",
       "                                           text_body publish_time  \\\n",
       "0  [{'text': 'The 2019 novel coronavirus (SARS-Co...   2020-03-12   \n",
       "\n",
       "                                             authors          journal  \\\n",
       "0  Bryson-Cahn, Chloe; Duchin, Jeffrey; Makarewic...  Clin Infect Dis   \n",
       "\n",
       "                   doi  H index  \n",
       "0  10.1093/cid/ciaa256      NaN  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A Novel Approach for a Novel Pathogen: using a home assessment team to evaluate patients for 2019 novel coronavirus (SARS-CoV-2)',\n",
       " 'Modeling the dynamics of novel coronavirus (2019-nCov) with fractional derivative']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(covid_df.head(2)['title'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Analysis (Phase 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now go through the papers to extract and rank excerpts that contain relevant information about risk factors for covid-19. \n",
    "We will do this through an analysis of:\n",
    "\n",
    "- Risk factors for covid-19\n",
    "- Study designs\n",
    "    - We will use this to evaluate the quality of a paper's methodologies for our rankings\n",
    "- Outcomes\n",
    "    - We will incentivise excerpts to explicitly mention outcomes that we have found researchers look for(in our interviews)\n",
    "- Fatality\n",
    "    - We have determined that information on mortality and fatality would be of high value to researcehrs, and rightly so.\n",
    "\n",
    "**Note**: The list of keywords were all obtained from a crowdsourced medical dictionary researchers had assembled. You can find more details [here](https://docs.google.com/spreadsheets/d/1t2e3CHGxHJBiFgHeW0dfwtvCG4x0CDCzcTFX7yz9Z2E/edit#gid=1217643351)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_factors = [{\n",
    "    'name': 'smoking',\n",
    "    'pattern': 'smok'\n",
    "}, {\n",
    "    'name': 'diabetes',\n",
    "    'pattern': 'diabete'\n",
    "}, {\n",
    "    'name': 'pregnancy',\n",
    "    'pattern': 'pregnan'\n",
    "}, {\n",
    "    'name': 'tuberculosis',\n",
    "    'pattern': '(tubercul|mtb|\\btb[A-Za-z0-9]\\b)'\n",
    "}, {\n",
    "    'name': 'hypertension',\n",
    "    'pattern': 'hypertension'\n",
    "}, {\n",
    "    'name': 'cancer',\n",
    "    'pattern': 'cancer'\n",
    "}, {\n",
    "    'name': 'neonates',\n",
    "    'pattern': '(baby|neonate|enfant)'\n",
    "},\n",
    "    {\n",
    "    'name': 'liver disease',\n",
    "    'pattern': 'liver disease'\n",
    "},{\n",
    "    'name': 'COPD',\n",
    "    'pattern': 'COPD'\n",
    "},{\n",
    "    'name': 'pulmonary disease',\n",
    "    'pattern': 'pulm'\n",
    "},{\n",
    "    'name': 'race/ethnicity',\n",
    "    'pattern': 'ethnic'\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_list = [\n",
    "    'mathemat', 'profil', 'cross sectional case control',\n",
    "    'matched case control', 'contact', 'surviv', 'tracing,', 'time to event',\n",
    "    'time-to-event', 'risk factor analysis', 'logistic regression',\n",
    "    'cross-sectional case-control', 'matched case-control',\n",
    "    'observational case series', 'time series analysis', 'survival analysis',\n",
    "    'investigati', 'model', 'outbreak', 'stochast', 'statist', 'analysi',\n",
    "    'experiment', 'excret', 'investig',\n",
    "    'retrospective cohort', 'cross-sectional case-control',\n",
    "    'cross sectional case control', 'prevalence survey', 'systematic review ',\n",
    "    'meta-analysis', 'meta analysis', 'matched case-control',\n",
    "    'matched case control', 'medical record review',\n",
    "    'observational case series', 'time series analysis',\n",
    "    'pseudo-randomized controlled', 'pseudo randomized controlled',\n",
    "    'randomized controlled', 'retrospective analysis', 'retrospective study',\n",
    "    'retrospective studies'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_list = [\n",
    "    'risk', 'range', 'duration', 'asymptomatic', 'infecti', 'reproducti',\n",
    "    'route', 'age', 'transmm'\n",
    "    'stratifi', 'period,', 'health', 'r0', 'shedd', 'viral'\n",
    "    'period', 'incub', 'generat', 'factor', 'interval,', 'serial'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "fatality_list = ['icu', 'fatal', 'death', 'die', 'dead', 'dying', 'mortal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The next cell will contain the default coeffecients for the algorithm's prioritization of different features. These coeffecients are a work in progress and we seek to constantly improve them with more expert feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_weights= {\n",
    "    'risk': 2,\n",
    "    'design': 1,\n",
    "    'outcome': 2,\n",
    "    'fatality': 1,\n",
    "    'section': 1,\n",
    "    'inverse_length': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `PaperAnalyzer` class will take in a DataFrame of papers and then analyze each paper. \n",
    "The analysis is done with with its `analyze_risks()`, `analyze_designs()` and `analyze_outcomes()` methods that will analyze the risk factors, designs and outcomes respectively for excerpts in the paper. Finally, the `get_df()` method will return a new DataFrame with rankings for relevancy of excerpts. The rankings also factor in the `section` of the paper that the excerpt is from, with sections like **discussion** or **results** that seem to have pertinent, concise information ranked higher. Furthermore, these rankings are also normalized by the lenght of the excerpts\n",
    "\n",
    "*Note: These rankings for sections were determined through our interviews with epidimiologists.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `PaperAnalyzer` helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_design(design_keyword):\n",
    "    \"\"\"\n",
    "    This helper function ranks study designs. So far we have\n",
    "    confirmed rankings for only three study designs, but this\n",
    "    data will be expanded and improved further with time as we \n",
    "    speak to more epidimiologists\n",
    "    \"\"\"\n",
    "    design_rankings = {\n",
    "        'meta': 10,\n",
    "        'random': 8,\n",
    "        'pseudo': 6,\n",
    "    }\n",
    "    current_ranking = -1\n",
    "    for key in design_rankings.keys():\n",
    "        if key in design_keyword.lower():\n",
    "            current_ranking = min(current_ranking, design_rankings[key])\n",
    "    \n",
    "    if current_ranking == -1:\n",
    "        current_ranking = 4\n",
    "    return current_ranking    \n",
    "\n",
    "def flatten(arr):\n",
    "    \"\"\"\n",
    "    Returns a single flat list from a list of lists\n",
    "    \"\"\"\n",
    "    return [item \n",
    "            for sublist in arr \n",
    "            for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperAnalyzer():    \n",
    "    \"\"\"\n",
    "    Takes in a dataframe of papers and set it up for analysis\n",
    "    \"\"\"\n",
    "    # Setting up static constants\n",
    "    DEFAULT_RISKS = risk_factors\n",
    "    DEFAULT_DESIGNS = design_list\n",
    "    DEFAULT_OUTCOMES = outcome_list\n",
    "    DEFAULT_FATAL = fatality_list\n",
    "    DEFAULT_WEIGHTS = evaluation_weights\n",
    "    \n",
    "    def __init__(self, parent_df, weights = None):\n",
    "        \"\"\"\n",
    "        Explodes the passed dataframe on sections for more granular analysis\n",
    "        Sets up ranks to be updated later by methods. Client can supply their\n",
    "        own dictionary of weights for different features.\n",
    "        \"\"\"\n",
    "        # Section ratings\n",
    "        self.section_ratings = {\n",
    "                        'discus': 10,\n",
    "                        'concl': 10,\n",
    "                        'resul': 10,\n",
    "                        'analy': 9,\n",
    "                        'impli': 9,\n",
    "                        'valu': 9,\n",
    "                        'intro': 6\n",
    "                        }\n",
    "        \n",
    "        parent_df['full_text'] = parent_df['text_body'].apply(lambda x: '\\n'.join([sec['text'] for sec in x]))\n",
    "        self.df = parent_df.explode('text_body')\n",
    "        # Extracting section headers\n",
    "        self.df['section'] = self.df['text_body'].apply(lambda x: \n",
    "                                                        x['section'] \n",
    "                                                        if type(x) == dict \n",
    "                                                        else None)\n",
    "        # Extracting section texts\n",
    "        self.df['text_body'] = self.df['text_body'].apply(lambda x:\n",
    "                                                          x['text'] \n",
    "                                                          if type(x) == dict \n",
    "                                                          else None)\n",
    "        # Dropping rows where section text is empty\n",
    "        self.df = self.df[self.df['text_body'].notna()]\n",
    "        self.df['total_rank'] = 0\n",
    "        if weights:\n",
    "            self.weights = weights\n",
    "        else:\n",
    "            self.weights = PaperAnalyzer.DEFAULT_WEIGHTS\n",
    "        # TQDM is used for progress bars\n",
    "        tqdm.pandas()\n",
    "\n",
    "    def analyze_risks(self, risk_factors):\n",
    "        \"\"\"\n",
    "        Analyses papers in self.df for risk factors and returns a report df\n",
    "        with columns has_{risk_factor}?, {risk_factor}_count, \n",
    "        {risk_factor}_in_title and updates {total_rank} for each row.\n",
    "        The match_indices column is produced for ease of visualization\n",
    "        in the web app.\n",
    "        \"\"\"\n",
    "        if risk_factors == None:\n",
    "            risk_factors = PaperAnalyzer.DEFAULT_RISKS\n",
    "        \n",
    "        if type(risk_factors[0]) == dict:\n",
    "            patterns = [risk['pattern'] for risk in risk_factors]\n",
    "        elif type(risk_factors[0]) == str:\n",
    "            patterns = [risk for risk in risk_factors]\n",
    "        \n",
    "        self.df = self.df[self.df['text_body'].apply(lambda x:\n",
    "                                                    any(re.compile(pattern, re.IGNORECASE).findall(x)\n",
    "                                                       for pattern in patterns)\n",
    "                                                    )]\n",
    "        self.df['risk_factors'] = [[]] * len(self.df)\n",
    "        self.df['match_indices'] = [[]] * len(self.df)\n",
    "        for i in tqdm(range(len(risk_factors))):\n",
    "            factor = risk_factors[i]\n",
    "            if type(factor) == dict:\n",
    "                name = factor['name']\n",
    "                pattern = re.compile(factor['pattern'], re.IGNORECASE)\n",
    "            elif type(factor) == str:\n",
    "                name = factor\n",
    "                pattern = re.compile(factor, re.IGNORECASE)\n",
    "    \n",
    "            self.df['_matches'] = self.df['text_body'].apply(lambda x: \n",
    "                                                                      [(m.start(), m.group()) \n",
    "                                                                       for m in pattern.finditer(x)])\n",
    "            \n",
    "            self.df[name + '_count'] = self.df['_matches'].apply(lambda x: len(x))\n",
    "            self.df['has_' + name + '?'] = self.df[name + '_count'].apply(lambda x: x > 0)\n",
    "            self.df[name + '_in_title'] = self.df['title'].apply(lambda x:\n",
    "                                                                         len(pattern.findall(x)) > 0)\n",
    "            self.df[name + '_count'] = self.df.apply(lambda x:\n",
    "                                                             x[name + '_count'] + 10\n",
    "                                                            if x[name + '_in_title'] \n",
    "                                                             else x[name + '_count'],\n",
    "                                                            axis=1)\n",
    "            self.df['total_rank'] += self.weights['risk'] * self.df[name + '_count']\n",
    "            \n",
    "            self.df['risk_factors'] = self.df.apply(lambda x: \n",
    "                                    x['risk_factors'] + [name] if x['has_' + name + '?']\n",
    "                                    else x['risk_factors'],\n",
    "                                   axis=1)\n",
    "            self.df['match_indices'] = self.df.apply(lambda x: \n",
    "                                    x['match_indices'] + [n[0] for n in x['_matches']] if x['has_' + name + '?']\n",
    "                                    else x['match_indices'],\n",
    "                                   axis=1)\n",
    "            self.df.drop('_matches', axis=1, inplace=True)\n",
    "            \n",
    "\n",
    "    def analyze_designs(self, design_list):\n",
    "        \"\"\"\n",
    "        Analyses papers in self.df for study designs and returns a report df \n",
    "        with 'design' and 'design_rank'. 'design_rank' is decided upon from the \n",
    "        input in crowdsourced medical dictionary.\n",
    "        \"\"\"\n",
    "        if design_list == None:\n",
    "            design_list = PaperAnalyzer.DEFAULT_DESIGNS\n",
    "        self.df['design'] = self.df['text_body'].progress_apply(lambda x:\n",
    "                                                                      [re.findall(des, x, re.IGNORECASE) \n",
    "                                                                       for des in design_list])\n",
    "        self.df['design_rank'] = self.df['design'].apply(lambda x:\n",
    "                                                                len(x))\n",
    "        self.df['design_rank'] += self.df['design'].apply(lambda x: rank_design(' '.join(flatten(x))))\n",
    "        self.df['total_rank'] += self.weights['design'] * self.df['design_rank']\n",
    "\n",
    "    def analyze_outcomes(self, outcomes):\n",
    "        \"\"\"\n",
    "        Analyses papers in self.df for outcomes and returns a report df \n",
    "        with 'outcomes' and 'outcome_rank'. 'outcome_rank' is decided upon \n",
    "        by the frequency of mentions of outcomes in the excerpt\n",
    "        \"\"\"\n",
    "        \n",
    "        if outcomes == None:\n",
    "            outcomes = PaperAnalyzer.DEFAULT_OUTCOMES\n",
    "        self.df['outcomes'] = self.df['text_body'].progress_apply(lambda x:\n",
    "                                                                        [re.findall(outcome, x, re.IGNORECASE)\n",
    "                                                                         for outcome in outcomes])\n",
    "        self.df['outcome_rank'] = self.df['outcomes'].apply(lambda x: len(x))\n",
    "        self.df['total_rank'] += self.weights['outcome']* self.df['outcome_rank']\n",
    "        \n",
    "    def analyze_fatality(self, fatality_list):\n",
    "        \"\"\"\n",
    "        Analyses papers in self.df for information on fatality \n",
    "        returns a report df with 'fatality_rank'. \n",
    "        'fatality_rank' is decided upon by the frequency of \n",
    "        mentions of fatality in the excerpt\n",
    "        \"\"\"\n",
    "        \n",
    "        if fatality_list == None:\n",
    "            fatality_list = PaperAnalyzer.DEFAULT_FATAL\n",
    "        self.df['fatality_rank'] = self.df['text_body'].progress_apply(lambda x:\n",
    "                                                                        len([re.findall(key, x, re.IGNORECASE)\n",
    "                                                                         for key in fatality_list]))\n",
    "        self.df['has_fatality_info?'] = self.df['fatality_rank'].apply(lambda x: x > 0)\n",
    "        self.df['total_rank'] += self.weights['fatality']* self.df['fatality_rank']\n",
    "\n",
    "    def perform_analysis(self, risk_factors, design_list=None, outcomes=None, fatality_list = None):\n",
    "        \"\"\"\n",
    "        This function is a wrapper function that provides interface\n",
    "        to conduct analysis on all of risk factors, study designs and\n",
    "        outcomes. Users may specify their own design_list or outcomes. If not,\n",
    "        the default is used.\n",
    "        \"\"\"\n",
    "        print(\"Analyzing risks\")\n",
    "        self.analyze_risks(risk_factors)\n",
    "        print(\"Analyzing study designs\")\n",
    "        self.analyze_designs(design_list)\n",
    "        print(\"Analyzing outcomes\")\n",
    "        self.analyze_outcomes(outcomes)\n",
    "        print(\"Analyzing fatality\")\n",
    "        self.analyze_fatality(fatality_list)\n",
    "\n",
    "    def get_df(self, risk_factor=None):\n",
    "        \"\"\"\n",
    "        Applies section ratings, updates total ratings and returns reporting df\n",
    "            risk_factor: if specified, the returned df will only have excerpts\n",
    "                            that mention this risk factor\n",
    "        \"\"\"\n",
    "        self.df['section_rank'] = self.df['section'].apply(lambda x: self.section_ratings[x] \n",
    "                                                           if x in self.section_ratings else 5)\n",
    "        self.df['total_rank'] += self.weights['section'] * self.df['section_rank']\n",
    "        \n",
    "        # Normalizing total rank for length of excerpts\n",
    "        self.df['total_rank'] = self.df.apply(lambda x: x['total_rank'] + \n",
    "                                                        (self.weights['inverse_length']/\n",
    "                                                         (len(word_tokenize(x['text_body'])))), \n",
    "                                              axis=1)\n",
    "        if risk_factor:\n",
    "            if not self.__ANALYZED_RISKS:\n",
    "                raise ValueError(self.__ERROR_MESSAGE)\n",
    "            return self.df[self.df['has_' + risk_factor + '?'] == True]\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "242c1d0e54c7477896557138a4573b74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62274faa8faa471c922cd76fb3914622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=652.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d015664c79847e7a8ea10031e90c6da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=652.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "741c6b4ad42f42d4b4100628cf683831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=652.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "covid_analysis = PaperAnalyzer(covid_df)\n",
    "covid_analysis.analyze_risks(risk_factors)\n",
    "covid_analysis.analyze_designs(design_list)\n",
    "covid_analysis.analyze_outcomes(outcome_list)\n",
    "covid_analysis.analyze_fatality(fatality_list)\n",
    "enriched_covid_df = covid_analysis.get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(652, 54)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enriched_covid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pneumonia arising from any infectious etiology is an important cause of morbidity and mortality among pregnant women. It is the most prevalent non-obstetric infectious condition that occurs during pregnancy [14] [15] [16] . In one study pneumonia was the 3rd most common cause of indirect maternal death [17] . Approximately 25 percent of pregnant women who develop pneumonia will need to be hospitalized in critical care units and require ventilatory support [16] . Although bacterial pneumonia is a serious disease when it occurs in pregnant women, even when the agent(s) are susceptible to antibiotics, viral pneumonia has even higher levels of morbidity and mortality during pregnancy [18] . As with other infectious diseases, the normal maternal physiologic changes that accompany pregnancy-including altered cell-mediated immunity [19] and changes in pulmonary function-have been hypothesized to affect both susceptibility to and clinical severity of pneumonia [20] [21] [22] . This has been evident historically during previous epidemics. The case fatality rate (CFR) for pregnant women infected with influenza during the 1918-1919 pandemic was 27%-even higher when exposure occurred during the 3rd trimester and upwards of 50% if pneumonia supervened [23] . During the 1957-1958 Asian flu epidemic, 10% of all deaths occurred in pregnant women, and their CFR was twice as high as that of infected women who were not pregnant [24] . The most common adverse obstetrical outcomes associated with maternal pneumonias from all causes include This newly recognized coronavirus, producing a disease that has been termed COVID-19, is rapidly spreading throughout China, has crossed international borders to infect persons in neighboring countries, and humans infected by the virus are travelling via commercial airlines to other continents. It is certain that 2019-nCoV will infect women who are pregnant, leaving the question open as to whether the novel coronavirus will have a similar or different effect on them compared with SARS-CoV and MERS-CoV. In order to address the potential obstetrical outcomes of infection to both mother and infant, the present communication describes the current state of knowledge regarding the effects of other coronavirus infections in pregnancy.\\nPneumonia arising from any infectious etiology is an important cause of morbidity and mortality among pregnant women. It is the most prevalent non-obstetric infectious condition that occurs during pregnancy [14] [15] [16] . In one study pneumonia was the 3rd most common cause of indirect maternal death [17] . Approximately 25 percent of pregnant women who develop pneumonia will need to be hospitalized in critical care units and require ventilatory support [16] . Although bacterial pneumonia is a serious disease when it occurs in pregnant women, even when the agent(s) are susceptible to antibiotics, viral pneumonia has even higher levels of morbidity and mortality during pregnancy [18] . As with other infectious diseases, the normal maternal physiologic changes that accompany pregnancy-including altered cell-mediated immunity [19] and changes in pulmonary function-have been hypothesized to affect both susceptibility to and clinical severity of pneumonia [20] [21] [22] . This has been evident historically during previous epidemics. The case fatality rate (CFR) for pregnant women infected with influenza during the 1918-1919 pandemic was 27%-even higher when exposure occurred during the 3rd trimester and upwards of 50% if pneumonia supervened [23] . During the 1957-1958 Asian flu epidemic, 10% of all deaths occurred in pregnant women, and their CFR was twice as high as that of infected women who were not pregnant [24] . The most common adverse obstetrical outcomes associated with maternal pneumonias from all causes include premature rupture of membranes (PROM) and preterm labor (PTL), intrauterine fetal demise (IUFD), intrauterine growth restriction (IUGR), and neonatal death [14] [15] [16] .'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enriched_covid_df.sort_values(by='total_rank', ascending=False).iloc[6]['text_body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_covid_df.to_json(\"../../enriched_covid_df.json\", orient='records')\n",
    "enriched_covid_df.to_csv(\"../../enriched_covid_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This marks the end of phase 1. The resulting dataframe will be stored as a json to be served by the web app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Search (Phase 2)\n",
    "We will extend the capabilities from the `PaperAnalyzer` class and attempt to answer some questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `Question` class will decompose and resolve a question about risk factors.\n",
    "The result will then be piped to an instance of `PaperAnalyzer` to conduct similar analysis. Users will be able to specify their own list of outcomes. If not specified, the default set of outcomes will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question():\n",
    "    \"\"\"\n",
    "    The purpose of this class is to resolve a question for \n",
    "    keyword searching\n",
    "    \"\"\"\n",
    "    def __init__(self, question, design_list=None, outcomes=None):\n",
    "        \"\"\"\n",
    "        The constuctor does most of the method-calling for question resolution\n",
    "        \"\"\"\n",
    "        self.DESIGN_LIST = design_list\n",
    "        self.OUTCOMES = outcomes\n",
    "        self.RISK = question\n",
    "        self.risk_factors = None\n",
    "        self.design_list = None\n",
    "        self.outcome_list = None\n",
    "        self.__resolve_question()\n",
    "        if design_list:\n",
    "            self.__resolve_design()\n",
    "        if outcomes:\n",
    "            self.__resolve_outcomes()\n",
    "\n",
    "    def __question_tokenize(self, sent):\n",
    "        \"\"\"\n",
    "        Cleans the question string\n",
    "        \"\"\"\n",
    "        abbvr_pattern = re.compile('(e.g.|i.e.)')\n",
    "        sent = abbvr_pattern.sub('', sent)\n",
    "        remove_punct_dict = {key: \" \" for key in string.punctuation}\n",
    "        remove_punct_dict['.'] = ''\n",
    "        remove_punct = str.maketrans(remove_punct_dict)\n",
    "        sent = sent.translate(remove_punct)\n",
    "        return sent.replace('R', 'R0').replace('-', ' ')\n",
    "\n",
    "    def __resolve_question(self):\n",
    "        \"\"\"\n",
    "        stems and removes irreleavnt words from questions\n",
    "        to create keywords for keyword analysis\n",
    "        \"\"\"\n",
    "        subquestion = self.RISK\n",
    "        sub_q = self.__question_tokenize(subquestion)\n",
    "        keywords = set([\n",
    "            stemmer.stem(word) for word in word_tokenize(sub_q)\n",
    "            if word.lower() not in stop_words and 'cov' not in word.lower()\n",
    "            and word.lower().islower()  #This checks and removes numbers\n",
    "        ])\n",
    "        self.risk_factors = list(keywords)\n",
    "\n",
    "    def __resolve_design(self):\n",
    "        \"\"\"\n",
    "        Resolves study designs to allow for study-design evaluation\n",
    "        \"\"\"\n",
    "        design_keys = self.DESIGN_LIST.split(\",\")\n",
    "        self.design_list = list(set(design_keys))\n",
    "\n",
    "    def __resolve_outcomes(self):\n",
    "        \"\"\"\n",
    "        Resolves outcomes to allow for outcome evaluation\n",
    "        \"\"\"\n",
    "        outcome_keys = self.__question_tokenize(self.OUTCOMES)\n",
    "        outcome_keys = set([\n",
    "            stemmer.stem(word) for word in word_tokenize(outcome_keys)\n",
    "            if word.lower() not in stop_words\n",
    "            and word.lower().islower()  #This checks and removes numbers\n",
    "        ])\n",
    "        self.outcome_list = list(outcome_keys)\n",
    "\n",
    "    def get_keywords(self):\n",
    "        \"\"\"\n",
    "        Returns keywords from earlier methods\n",
    "        \"\"\"\n",
    "        result = {'risk': None, 'design': None, 'outcome': None}\n",
    "        result['risk'] = self.risk_factors\n",
    "        if self.design_list:\n",
    "            result['design'] = self.design_list\n",
    "        if self.outcome_list:\n",
    "            result['outcome'] = self.outcome_list\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_question(df, question):\n",
    "    \"\"\"\n",
    "    Function to take in a Question instance and a \n",
    "    dataframe with covid-excerpts to perform \n",
    "    evaluation and rankings on information relevancy\n",
    "    \"\"\"\n",
    "    reference_df = PaperAnalyzer(df)\n",
    "    keys = question.get_keywords()\n",
    "    reference_df.perform_analysis(keys['risk'], keys['design'],\n",
    "                                  keys['outcome'])\n",
    "    return reference_df.get_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will be trying out the question answering pipeline now with a few questions from the aforementionned [medical dictionary](https://docs.google.com/spreadsheets/d/1t2e3CHGxHJBiFgHeW0dfwtvCG4x0CDCzcTFX7yz9Z2E/edit#gid=1217643351)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_google_sheet(url, sheet_name):\n",
    "    response=requests.get(url=url)\n",
    "    sample_file = io.BytesIO(response.content)\n",
    "    df = pd.read_excel(sample_file, sheet_name = sheet_name)\n",
    "    return df\n",
    "\n",
    "dict_url = 'https://docs.google.com/spreadsheets/d/1t2e3CHGxHJBiFgHeW0dfwtvCG4x0CDCzcTFX7yz9Z2E/export?format=xlsx&id=1t2e3CHGxHJBiFgHeW0dfwtvCG4x0CDCzcTFX7yz9Z2E'\n",
    "questions_df = get_google_sheet(dict_url, 'sub.question.matching')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Question</th>\n",
       "      <th>Subquestion</th>\n",
       "      <th>Outcome.list</th>\n",
       "      <th>Differences.list</th>\n",
       "      <th>Design.list</th>\n",
       "      <th>Notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is known about transmission, incubation, ...</td>\n",
       "      <td>Range of incubation periods for the disease in...</td>\n",
       "      <td>Range of incubation periods for humans: genera...</td>\n",
       "      <td>incubation period</td>\n",
       "      <td>age.</td>\n",
       "      <td>contact tracing, survival analysis, time-to-ev...</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is known about transmission, incubation, ...</td>\n",
       "      <td>Range of incubation periods for the disease in...</td>\n",
       "      <td>Range of incubation periods for humans: by age...</td>\n",
       "      <td>incubation period, stratified by age group</td>\n",
       "      <td>age.</td>\n",
       "      <td>contact tracing, survival analysis, time-to-ev...</td>\n",
       "      <td>Notes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Unnamed: 0  \\\n",
       "0  What is known about transmission, incubation, ...   \n",
       "1  What is known about transmission, incubation, ...   \n",
       "\n",
       "                                            Question  \\\n",
       "0  Range of incubation periods for the disease in...   \n",
       "1  Range of incubation periods for the disease in...   \n",
       "\n",
       "                                         Subquestion  \\\n",
       "0  Range of incubation periods for humans: genera...   \n",
       "1  Range of incubation periods for humans: by age...   \n",
       "\n",
       "                                 Outcome.list Differences.list  \\\n",
       "0                           incubation period             age.   \n",
       "1  incubation period, stratified by age group             age.   \n",
       "\n",
       "                                         Design.list  \\\n",
       "0  contact tracing, survival analysis, time-to-ev...   \n",
       "1  contact tracing, survival analysis, time-to-ev...   \n",
       "\n",
       "                                               Notes  \n",
       "0  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4...  \n",
       "1                                              Notes  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is COVID-19 transmitted on droplets?'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Picking a sample question for analysis\n",
    "ques = questions_df.iloc[127]['Subquestion']\n",
    "ques = sent_tokenize(ques)[0]\n",
    "ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'risk factor analysis, logistic regression, cross-sectional case-control, matched case-control, observational case series, time series analysis, survival analysis'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Designs recommended for sample question\n",
    "des = questions_df.iloc[127]['Design.list']\n",
    "des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'odds of COVID-19 acquisition by occupation, age group, PPE use, observed/self-reported risk behaviors (e.g., inappropriately lowering mask to speak, touching face, eating without washing hands)'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Outcomes recommended for sample question\n",
    "outc = questions_df.iloc[127]['Outcome.list']\n",
    "outc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing risks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98ecc4d4f7584f5d929ebdb39b1fff95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing study designs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d15a098daf46bc88177bf6b93afd25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=304.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing outcomes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28201447ee544559ac05160fb01bb0bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=304.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing fatality\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f866c1aaa7f2461283b0499610b23563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=304.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "report_df = analyze_question(covid_df, Question(ques, des, outc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will now look at the 5 most relevant excerpts to the question we picked earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COVID-19 requires a different strategic approach to SARS, most notably because of the exceptionally large numbers of hospitalised patients and international pace of transmission. The main transmission risks for all coronaviruses are exposure to droplets and contact transfer of virus. Droplets are heavy and usually disperse within a maximum 2 metre radius following coughing and sneezing by an infected patient. A notable exception is when we carry out procedures with patients that generate artificial aerosols (small particle nuclei, which may penetrate standard surgical masks and contaminate a wider dispersal radius). 12 Aerosol generating procedures include, for example, tracheal intubation, non-invasive ventilation, high-flow nasal oxygen provision, bronchial suctioning, bronchoscopy, and sputum induction. These procedures place clinicians in close proximity to the upper respiratory tract source of viral load and which have been linked to an increased risk of transmission of coronaviru\n",
      "----------------------------- \n",
      "\n",
      "Repeated recent outbreaks of novel infectious diseases (e.g. H1N1 pdm09, SARS, MERS, avian influenza H5N1, Ebola) that have failed to develop into globe-spanning pandemics have given the international community a sense of security. Coronavirus disease (COVID-19) is undermining this sense of security because it is proving both highly transmissible (like H1N1) and not too deadly (unlike Ebola). The result is a relatively easily transmitted virus with a case fatality rate currently estimated as well above that of the seasonal influenza. With its high transmissibility and rapid global circulation, COVID-19 is being compared with the deadly Spanish flu outbreak of 1918. 1 On 30 January, 2020, the World Health Organization (WHO) declared COVID-19 a public health emergency of international concern. Since erupting in China, it has claimed thousands of lives there, and has now spread globally, causing infections, death and growing alarm. The WHO's initial focus was on managing droplet and conta\n",
      "----------------------------- \n",
      "\n",
      "COVID-19 requires a different strategic approach to SARS, most notably because of the exceptionally large numbers of hospitalised patients and international pace of transmission.\n",
      "On 30th January 2020 the WHO announced that the COVID-19 outbreak was a Public Health 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59 The main transmission risks for all coronaviruses are exposure to droplets and contact transfer of virus. Droplets are heavy and usually disperse within a maximum 2 metre radius following coughing and sneezing by an infected patient. A notable exception is when we carry out procedures with patients that generate artificial aerosols (small particle nuclei, which may penetrate standard surgical masks and contaminate a wider dispersal radius). 12 Aerosol generating procedures (AGPs) include, for example, tr\n",
      "----------------------------- \n",
      "\n",
      "We collected patient-level data for 2015 Covid-19 cases reported between January 1 and February 25, 2020 (Table 1, Supplementary Table 1) . 887 (44.0%) cases were female adults with a median age of 46 years (IQR 41.5-50.5), 1029 (51.0%) were male adults and the median age was 45 years (IQR 40.5-50.5), and 99 (4.9 %) cases were children with the median age was 9 years (IQR 7-11) with a boy-to-girl ratio of 1.3:1. 774 (38.4%) were imported cases, most reported a travel history to Wuhan (133, 17.2%) or were residents of Wuhan (641, 82.8%), while 240 (11.9%) of the imported case were either the residents or returning travelers from non-Wuhan regions.\n",
      "The incubation period is essential in the control of infectious diseases. To halt a pandemic, a suitable quarantine period should be set by investigating incubation periods. Thus, understanding the characteristics of the incubation period is crucial to design public health efforts. In this cohort, the full range of incubation periods of the Co\n",
      "----------------------------- \n",
      "\n",
      "Radiographers are among the first-line health care workers who might be exposed to 2019 novel COVID-19. Diagnostic imaging facilities should have guidelines in place to manage individuals with known or suspected COVID-19 infection. The novel COVID-19 is highly contagious and is believed to transmit mostly through respiratory droplets, but there is uncertainty as to whether the virus can be transmitted by touching a surface or an item that is contaminated (ie, a fomite). A thorough understanding of the routes of virus transmission will be essential for patients' and health care professionals' safety. Droplets have the greatest risk of transmission within 3 ft (91.44 cm), but they may travel up to 6 ft (183 cm) from their source [19] . For the purpose of diagnostic imaging in individuals with NCIP, whenever possible, portable radiographic equipment should be used to limit transportation of patients. On the basis of experience with SARS, the use of a satellite radiography center and dedic\n",
      "----------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(report_df.sort_values(by='total_rank', ascending = False).iloc[i]['text_body'][:1000])\n",
    "    print(\"-----------------------------\",\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('anaconda3': virtualenv)",
   "language": "python",
   "name": "python37464bitanaconda3virtualenv11568a5b709c405b925f37b6b0b6dbdb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
