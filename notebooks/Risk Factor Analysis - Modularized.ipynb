{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import json\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "root = '../../kaggle_data/'\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "stop_words.extend(['within', 'what', 'how', 'eg', 'ie'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `PaperLoader` class will load all papers for the challenge and provide an interface for us to obtain `Pandas Dataframes` to work with. The focus will be on:\n",
    "- Obtaining Paper title, Abstract, Body\n",
    "- Obtaining Authors, Journal of Publication, Publication Date and Publication Date\n",
    "- Obtaining journal ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperLoader():\n",
    "    \"\"\"\n",
    "    Initializes PaperLoader class to read all .json files from root_directory\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, no_bib=True):\n",
    "        self.ROOT_DIR = root_dir\n",
    "        self.JSON_FILES = glob.glob(f'{root}/**/*.json', recursive=True)\n",
    "        self.PAPERS_COLUMN = {\n",
    "            \"doc_id\": [None],\n",
    "            \"title\": [None],\n",
    "            \"abstract\": [None],\n",
    "            \"text_body\": [None]\n",
    "        }\n",
    "        self.PAPERS_DF = None\n",
    "        self.NO_BIB = no_bib\n",
    "\n",
    "    \"\"\"\n",
    "    Removes sections with more than 5 URL/DOI/HTTP instances\n",
    "    \"\"\"\n",
    "\n",
    "    def __clean_bib(self, body_text):\n",
    "        merged_body = []\n",
    "        for segment in body_text:\n",
    "            if len(merged_body) > 0:\n",
    "                if merged_body[-1]['section'] == segment['section']:\n",
    "                    merged_body[-1]['text'] += '\\n' + segment['text']\n",
    "                    continue\n",
    "            merged_body.append(segment)\n",
    "\n",
    "        merged_body = [\n",
    "            segment for segment in merged_body\n",
    "            if len(re.findall(\"(http|doi|www)\", segment['text'])) <= 4\n",
    "        ]\n",
    "        return merged_body\n",
    "\n",
    "    \"\"\"\n",
    "    Creates a Pandas DataFrame from all json files in root_directory\n",
    "    Each json file represents a paper. \n",
    "    Features extracted are: doc_id, title, abstract, text_body\n",
    "    \"\"\"\n",
    "\n",
    "    def create_paper_df(self):\n",
    "        self.PAPERS_DF = pd.DataFrame.from_dict(self.PAPERS_COLUMN)\n",
    "        df_list = []\n",
    "        for i in tqdm(range(len(self.JSON_FILES))):\n",
    "            file_name = self.JSON_FILES[i]\n",
    "            row = {x: None for x in self.PAPERS_COLUMN}\n",
    "\n",
    "            with open(file_name) as json_data:\n",
    "                data = json.load(json_data)\n",
    "\n",
    "                doc_id = data['paper_id']\n",
    "                row['doc_id'] = doc_id\n",
    "                row['title'] = data['metadata']['title']\n",
    "                if len(row['title']) <= 1:\n",
    "                    continue\n",
    "\n",
    "                # Now need all of abstract. Put it all in\n",
    "                # a list then use str.join() to split it\n",
    "                # into paragraphs.\n",
    "\n",
    "                if ('abstract' not in data or 'body_text' not in data):\n",
    "                    continue\n",
    "                else:\n",
    "                    abstract_list = [abst['text'] for abst in data['abstract']]\n",
    "                    abstract = \"\\n \".join(abstract_list)\n",
    "\n",
    "                if len(abstract) <= 1:\n",
    "                    continue\n",
    "\n",
    "                row['abstract'] = abstract\n",
    "\n",
    "                # And lastly the body of the text.\n",
    "                # These clauses verify whether the user wants to clean up references\n",
    "                if self.NO_BIB:\n",
    "                    body_list = self.__clean_bib(data['body_text'])\n",
    "                else:\n",
    "                    body_list = [bt for bt in data['body_text']]\n",
    "\n",
    "                row['text_body'] = body_list\n",
    "\n",
    "                df_list.append(row)\n",
    "        self.PAPERS_DF = pd.DataFrame(df_list)\n",
    "\n",
    "    \"\"\"\n",
    "    Joins paper information with information on journal for paper,\n",
    "    authors, doi and published date\n",
    "    \"\"\"\n",
    "\n",
    "    def merge_metadata(self):\n",
    "        metadata_df = pd.read_csv(self.ROOT_DIR + 'metadata.csv')\n",
    "        metadata_df_for_join = metadata_df.loc[:, [\n",
    "            'sha', 'publish_time', 'authors', 'journal', 'doi'\n",
    "        ]]\n",
    "        self.PAPERS_DF = self.PAPERS_DF.merge(metadata_df_for_join,\n",
    "                                              left_on='doc_id',\n",
    "                                              right_on='sha',\n",
    "                                              how='inner')\n",
    "\n",
    "    \"\"\"\n",
    "    Joins paper information with information on journal ratings\n",
    "    Important column: H_Index\n",
    "    \"\"\"\n",
    "\n",
    "    def merge_journals(self):\n",
    "        journal_df = pd.read_csv(root + 'scimagoj_2018.csv', sep=';')\n",
    "        papers_ratings_df = self.PAPERS_DF.merge(\n",
    "            journal_df.loc[:, ['Title', 'H index']],\n",
    "            left_on='journal',\n",
    "            right_on='Title',\n",
    "            how='left')\n",
    "        papers_ratings_df = papers_ratings_df.drop(\n",
    "            ['sha', 'Title'], axis=1).reset_index(drop=True)\n",
    "        self.PAPERS_DF = papers_ratings_df\n",
    "\n",
    "    def get_df(self):\n",
    "        self.PAPERS_DF = self.PAPERS_DF.dropna(\n",
    "            subset=['abstract', 'text_body'])\n",
    "        return self.PAPERS_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e479717868646aeb13c86ecd3920e61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=59311.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "paper_loader = PaperLoader(root)\n",
    "paper_loader.create_paper_df()\n",
    "paper_loader.merge_metadata()\n",
    "paper_loader.merge_journals()\n",
    "papers_df = paper_loader.get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>text_body</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>doi</th>\n",
       "      <th>H index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>306ef95a3a91e13a93bcc37fb2c509b67c0b5640</td>\n",
       "      <td>A Novel Approach for a Novel Pathogen: using a...</td>\n",
       "      <td>Thousands of people in the United States have ...</td>\n",
       "      <td>[{'text': 'The 2019 novel coronavirus (SARS-Co...</td>\n",
       "      <td>2020-03-12</td>\n",
       "      <td>Bryson-Cahn, Chloe; Duchin, Jeffrey; Makarewic...</td>\n",
       "      <td>Clin Infect Dis</td>\n",
       "      <td>10.1093/cid/ciaa256</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6599ebbef3d868afac9daa4f80fa075675cf03bc</td>\n",
       "      <td>International aviation emissions to 2025: Can ...</td>\n",
       "      <td>International aviation is growing rapidly, res...</td>\n",
       "      <td>[{'text': 'Sixty years ago, civil aviation was...</td>\n",
       "      <td>2009-01-31</td>\n",
       "      <td>Macintosh, Andrew; Wallace, Lailey</td>\n",
       "      <td>Energy Policy</td>\n",
       "      <td>10.1016/j.enpol.2008.08.029</td>\n",
       "      <td>178.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     doc_id  \\\n",
       "0  306ef95a3a91e13a93bcc37fb2c509b67c0b5640   \n",
       "1  6599ebbef3d868afac9daa4f80fa075675cf03bc   \n",
       "\n",
       "                                               title  \\\n",
       "0  A Novel Approach for a Novel Pathogen: using a...   \n",
       "1  International aviation emissions to 2025: Can ...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Thousands of people in the United States have ...   \n",
       "1  International aviation is growing rapidly, res...   \n",
       "\n",
       "                                           text_body publish_time  \\\n",
       "0  [{'text': 'The 2019 novel coronavirus (SARS-Co...   2020-03-12   \n",
       "1  [{'text': 'Sixty years ago, civil aviation was...   2009-01-31   \n",
       "\n",
       "                                             authors          journal  \\\n",
       "0  Bryson-Cahn, Chloe; Duchin, Jeffrey; Makarewic...  Clin Infect Dis   \n",
       "1                 Macintosh, Andrew; Wallace, Lailey    Energy Policy   \n",
       "\n",
       "                           doi  H index  \n",
       "0          10.1093/cid/ciaa256      NaN  \n",
       "1  10.1016/j.enpol.2008.08.029    178.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25323, 9)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering for covid-19 related papers released after 2019\n",
    "There is a lot of noise in this dataset due to information about other strains of coronavirus so we will select only the papers that are related to Covid-19. \n",
    "\n",
    "\n",
    "Also, while the older papers may contain some important insight on the variance among the  different strains of coronavirus, for our purposes, we will only be looking at papers published on 2019 or later because that is when Covid-19 became popular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of keywords for covid-19\n",
    "cov_list = [\n",
    "    'novel coronavi',\n",
    "    'covid',\n",
    "    'cov_2',\n",
    "    'cord-19',\n",
    "    'cord 19',\n",
    "    '2019-nCoV',\n",
    "    '2019 ncov',\n",
    "    '2019 cov',\n",
    "    'wuhan coronavi',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `RelevantFilter` class will filter the `DataFrame` from `PaperLoader` and filter for covid-19 papers published on 2019 or later. \n",
    "We will need to supply a list of covid-related keywords to filter from to the `constructor`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelevantFilter():\n",
    "    \"\"\"\n",
    "    Takes a list of key-words that the other methods use\n",
    "    to filter for relevant papers\n",
    "    \"\"\"\n",
    "    def __init__(self, keywords, year='2019'):\n",
    "        self.KEYWORDS = keywords\n",
    "        self.YEAR = year\n",
    "\n",
    "    def extract_recent(self, df):\n",
    "        return df[df['publish_time'] >= self.YEAR]\n",
    "\n",
    "    def filter_papers(self, df):\n",
    "        cov_pattern = re.compile('(' + \"|\".join(self.KEYWORDS) + ')',\n",
    "                                 re.IGNORECASE)\n",
    "        df = df[df['title'].apply(lambda x: len(cov_pattern.findall(x)) >= 1\n",
    "                                  if x else False)]\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_filter = RelevantFilter(cov_list, '2019')\n",
    "covid_df = covid_filter.filter_papers(papers_df)\n",
    "covid_df = covid_filter.extract_recent(covid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(932, 9)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>text_body</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>doi</th>\n",
       "      <th>H index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>306ef95a3a91e13a93bcc37fb2c509b67c0b5640</td>\n",
       "      <td>A Novel Approach for a Novel Pathogen: using a...</td>\n",
       "      <td>Thousands of people in the United States have ...</td>\n",
       "      <td>[{'text': 'The 2019 novel coronavirus (SARS-Co...</td>\n",
       "      <td>2020-03-12</td>\n",
       "      <td>Bryson-Cahn, Chloe; Duchin, Jeffrey; Makarewic...</td>\n",
       "      <td>Clin Infect Dis</td>\n",
       "      <td>10.1093/cid/ciaa256</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     doc_id  \\\n",
       "0  306ef95a3a91e13a93bcc37fb2c509b67c0b5640   \n",
       "\n",
       "                                               title  \\\n",
       "0  A Novel Approach for a Novel Pathogen: using a...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Thousands of people in the United States have ...   \n",
       "\n",
       "                                           text_body publish_time  \\\n",
       "0  [{'text': 'The 2019 novel coronavirus (SARS-Co...   2020-03-12   \n",
       "\n",
       "                                             authors          journal  \\\n",
       "0  Bryson-Cahn, Chloe; Duchin, Jeffrey; Makarewic...  Clin Infect Dis   \n",
       "\n",
       "                   doi  H index  \n",
       "0  10.1093/cid/ciaa256      NaN  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A Novel Approach for a Novel Pathogen: using a home assessment team to evaluate patients for 2019 novel coronavirus (SARS-CoV-2)',\n",
       " 'Modeling the dynamics of novel coronavirus (2019-nCov) with fractional derivative']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(covid_df.head(2)['title'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_factors = [{\n",
    "    'name': 'smoking',\n",
    "    'pattern': 'smok'\n",
    "}, {\n",
    "    'name': 'diabetes',\n",
    "    'pattern': 'diabete'\n",
    "}, {\n",
    "    'name': 'pregnancy',\n",
    "    'pattern': 'pregnan'\n",
    "}, {\n",
    "    'name': 'tuberculosis',\n",
    "    'pattern': '(tubercul|MTB|TB)'\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_list = [\n",
    "    'mathemat', 'profil', 'cross sectional case control',\n",
    "    'matched case control', 'contact', 'surviv', 'tracing,', 'time to event',\n",
    "    'time-to-event', 'risk factor analysis', 'logistic regression',\n",
    "    'cross-sectional case-control', 'matched case-control',\n",
    "    'observational case series', 'time series analysis', 'survival analysis',\n",
    "    'investigati', 'model', 'outbreak', 'stochast', 'statist', 'analysi',\n",
    "    'experiment', 'excret', 'investig'\n",
    "    'retrospective cohort', 'cross-sectional case-control',\n",
    "    'cross sectional case control', 'prevalence survey', 'systematic review ',\n",
    "    'meta-analysis', 'meta analysis', 'matched case-control',\n",
    "    'matched case control', 'medical record review',\n",
    "    'observational case series', 'time series analysis',\n",
    "    'pseudo-randomized controlled', 'pseudo randomized controlled',\n",
    "    'randomized controlled', 'retrospective analysis', 'retrospective study',\n",
    "    'retrospective studies'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_list = [\n",
    "    'risk', 'range', 'duration', 'asymptomatic', 'infecti', 'reproducti',\n",
    "    'route', 'age', 'transmm'\n",
    "    'stratifi', 'period,', 'health', 'r0', 'shedd', 'viral'\n",
    "    'period', 'incub', 'generat', 'factor', 'interval,', 'serial'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `PaperAnalyzer` \n",
    "Will be filled with description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperAnalyzer():\n",
    "    def __init__(self, parent_df):\n",
    "        self.section_ratings = {\n",
    "                        'discus': 10,\n",
    "                        'concl': 10,\n",
    "                        'resul': 10,\n",
    "                        'analy': 9,\n",
    "                        'impli': 9,\n",
    "                        'valu': 9,\n",
    "                        'intro': 6\n",
    "                        }\n",
    "        \n",
    "        self.df = parent_df.explode('text_body')\n",
    "        self.df['section'] = self.df['text_body'].apply(\n",
    "            lambda x: x['section'] if type(x) == dict else None)\n",
    "        self.df['text_body'] = self.df['text_body'].apply(\n",
    "            lambda x: x['text'] if type(x) == dict else None)\n",
    "        self.df = self.df[self.df['text_body'].notna()]\n",
    "        self.df['total_rank'] = 0\n",
    "        self.__ANALYZED_RISKS = False\n",
    "        self.__RISK_MESSAGE = \"Need to call analyze_risk()  first. Try calling self.analyze_risks()\"\n",
    "        tqdm.pandas()\n",
    "\n",
    "    def analyze_risks(self, risk_factors):\n",
    "        if type(risk_factors[0]) == dict:\n",
    "            patterns = [risk['patterns'] for risk in risk_factors]\n",
    "        elif type(risk_factors[0]) == str:\n",
    "            patterns = [risk for risk in risk_factors]\n",
    "        \n",
    "        self.df = self.df[self.df['text_body'].apply(lambda x:\n",
    "                                                    any(re.compile(pattern, re.IGNORECASE).findall(x)\n",
    "                                                       for pattern in patterns)\n",
    "                                                    )]\n",
    "        for i in tqdm(range(len(risk_factors))):\n",
    "            factor = risk_factors[i]\n",
    "            if type(factor) == dict:\n",
    "                name = factor['name']\n",
    "                pattern = factor['pattern']\n",
    "            elif type(factor) == str:\n",
    "                name = factor\n",
    "                pattern = factor\n",
    "            self.df[name + '_count'] = self.df['text_body'].apply(lambda x: \n",
    "                                                                          len(re.findall(pattern, x)))\n",
    "            self.df['has_' + name + '?'] = self.df[name + '_count'].apply(lambda x: x > 0)\n",
    "            self.df[name + '_in_title'] = self.df['title'].apply(lambda x:\n",
    "                                                                         len(re.findall(pattern, x)) > 0)\n",
    "            self.df[name + '_count'] = self.df.apply(lambda x:\n",
    "                                                             x[name + '_count'] + 10\n",
    "                                                            if x[name + '_in_title'] \n",
    "                                                             else x[name + '_count'],\n",
    "                                                            axis=1)\n",
    "            self.df['total_rank'] += self.df[name + '_count']\n",
    "        self.__ANALYZED_RISKS = True\n",
    "\n",
    "    def analyze_designs(self, design_list):\n",
    "        if not self.__ANALYZED_RISKS:\n",
    "            raise ValueError(self.__ERROR_MESSAGE)\n",
    "\n",
    "        self.df['design_matches'] = self.df['text_body'].progress_apply(lambda x:\n",
    "                                                                      [re.findall(des, x) \n",
    "                                                                       for des in design_list])\n",
    "        self.df['design_rank'] = self.df['design_matches'].apply(lambda x:\n",
    "                                                                        len(x))\n",
    "        self.df['total_rank'] += self.df['design_rank']\n",
    "\n",
    "    def analyze_outcomes(self, outcomes):\n",
    "        if not self.__ANALYZED_RISKS:\n",
    "            raise ValueError(self.__ERROR_MESSAGE)\n",
    "        self.df['outcome_matches'] = self.df['text_body'].progress_apply(lambda x:\n",
    "                                                                        [re.findall(outcome, x)\n",
    "                                                                         for outcome in outcomes])\n",
    "        self.df['outcome_rank'] = self.df['outcome_matches'].apply(lambda x: len(x))\n",
    "        self.df['total_rank'] += self.df['outcome_rank']\n",
    "\n",
    "    def perform_analysis(self, risk_factors, design_list=None, outcomes=None):\n",
    "        print(\"Analyzing risks\")\n",
    "        self.analyze_risks(risk_factors)\n",
    "        print(\"Analyzing study designs\")\n",
    "        self.analyze_designs(design_list)\n",
    "        print(\"Analyzing outcomes\")\n",
    "        self.analyze_outcomes(outcomes)\n",
    "\n",
    "    def get_df(self, risk_factor=None):\n",
    "        self.df['section_rank'] = self.df['section'].apply(lambda x: self.section_ratings[x] \n",
    "                                                           if x in self.section_ratings else 5)\n",
    "        self.df['total_rank'] += self.df['section_rank']\n",
    "        self.df['total_rank'] = self.df.apply(lambda x: x['total_rank']\n",
    "                                                     /len(word_tokenize(x['text_body'])), \n",
    "                                              axis=1)\n",
    "        if risk_factor:\n",
    "            if not self.__ANALYZED_RISKS:\n",
    "                raise ValueError(self.__ERROR_MESSAGE)\n",
    "            return self.df[self.df['has_' + risk_factor +\n",
    "                                                   '?'] == True]\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_analysis = PaperAnalyzer(covid_df)\n",
    "covid_analysis.analyze_risks(risk_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_analysis.analyze_designs(design_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_analysis.analyze_outcomes(outcome_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_covid_df = covid_analysis.get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_covid_df.to_json(\"../../enriched_covid_df.json\", orient='records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Question`\n",
    "Will be filled with description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_document = pd.read_csv(\n",
    "    '../../../Downloads/Kaggle COVID-19 medical dictionary - sub.question.matching.csv',\n",
    "    index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question():\n",
    "    def __init__(self, question, design_list=None, outcomes=None):\n",
    "        self.DESIGN_LIST = design_list\n",
    "        self.OUTCOMES = outcomes\n",
    "        self.RISK = question\n",
    "        self.risk_factors = None\n",
    "        self.design_list = None\n",
    "        self.outcome_list = None\n",
    "        self.__resolve_question()\n",
    "        if design_list:\n",
    "            self.__resolve_design()\n",
    "        if outcomes:\n",
    "            self.__resolve_outcomes()\n",
    "\n",
    "    def __cov_tokenize(self, sent):\n",
    "        abbvr_pattern = re.compile('(e.g.|i.e.)')\n",
    "        sent = abbvr_pattern.sub('', sent)\n",
    "        remove_punct = str.maketrans({key: \" \" for key in string.punctuation})\n",
    "        sent = sent.translate(remove_punct)\n",
    "        return sent.replace('R', 'R0').replace('-', ' ')\n",
    "\n",
    "    def __resolve_question(self):\n",
    "        subquestion = self.RISK\n",
    "        sub_q = self.__cov_tokenize(subquestion)\n",
    "        keywords = set([\n",
    "            stemmer.stem(word) for word in word_tokenize(sub_q)\n",
    "            if word.lower() not in stop_words and 'cov' not in word.lower()\n",
    "            and word.lower().islower()  #This checks and removes numbers\n",
    "        ])\n",
    "        self.risk_factors = list(keywords)\n",
    "\n",
    "    def __resolve_design(self):\n",
    "        design_keys = self.__cov_tokenize(self.DESIGN_LIST)\n",
    "        design_keys = set([\n",
    "            stemmer.stem(word) for word in word_tokenize(design_keys)\n",
    "            if word.lower() not in stop_words\n",
    "            and word.lower().islower()  #This checks and removes numbers\n",
    "        ])\n",
    "        self.design_list = list(design_keys)\n",
    "\n",
    "    def __resolve_outcomes(self):\n",
    "        outcome_keys = self.__cov_tokenize(self.OUTCOMES)\n",
    "        outcome_keys = set([\n",
    "            stemmer.stem(word) for word in word_tokenize(outcome_keys)\n",
    "            if word.lower() not in stop_words\n",
    "            and word.lower().islower()  #This checks and removes numbers\n",
    "        ])\n",
    "        self.outcome_list = list(outcome_keys)\n",
    "\n",
    "    def get_keywords(self):\n",
    "        result = {'risk': None, 'design': None, 'outcome': None}\n",
    "        result['risk'] = self.risk_factors\n",
    "        if self.design_list:\n",
    "            result['design'] = self.design_list\n",
    "        if self.outcome_list:\n",
    "            result['outcome'] = self.outcome_list\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_question(df, question):\n",
    "    reference_df = PaperAnalyzer(df)\n",
    "    keys = question.get_keywords()\n",
    "    reference_df.perform_analysis(keys['risk'], keys['design'],\n",
    "                                  keys['outcome'])\n",
    "    return reference_df.get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_q = question_document.iloc[127]['Subquestion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is COVID-19 transmitted on droplets?'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_q = sent_tokenize(sample_q)[0]\n",
    "sample_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('risk factor analysis, logistic regression, cross-sectional case-control, matched case-control, observational case series, time series analysis, survival analysis',\n",
       " str)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_d = question_document.iloc[127]['Design.list']\n",
    "sample_d, type(sample_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('odds of COVID-19 acquisition by occupation, age group, PPE use, observed/self-reported risk behaviors (e.g., inappropriately lowering mask to speak, touching face, eating without washing hands)',\n",
       " str)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_o = question_document.iloc[127]['Outcome.list']\n",
    "sample_o, type(sample_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing risks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3fef250f064cf88a5102fb782381f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing study designs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f4d6c20cb94bebb30998d29c74bc8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=305.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing outcomes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a3c484327e643d0a66d3a9250a10baa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=305.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "question_df = analyze_question(covid_df, Question(sample_q, sample_d,\n",
    "                                                  sample_o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For all suspected and confirmed cases, droplet precautions should be implemented, at a minimum. Staff must wear the following items:\\n• surgical mask • fluid-resistant long-sleeved gown • goggles or face shield • gloves 22'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_df.sort_values(by='total_rank', ascending = False).iloc[0]['text_body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_df['section'].value_counts()[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Show the sentence before keyword is happening\n",
    "- Show the sentence after keyword is happening\n",
    "- Top and bottom expansion happening\n",
    "- If it is smaller than certain # of words, just show entire excerpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('anaconda3': virtualenv)",
   "language": "python",
   "name": "python37464bitanaconda3virtualenv11568a5b709c405b925f37b6b0b6dbdb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
