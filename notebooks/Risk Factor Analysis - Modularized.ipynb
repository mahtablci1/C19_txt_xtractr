{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "root = '../../kaggle_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `PaperLoader` class will load all papers for the challenge and provide an interface for us to obtain `Pandas Dataframes` to work with. The focus will be on:\n",
    "- Obtaining Paper title, Abstract, Body\n",
    "- Obtaining Authors, Journal of Publication, Publication Date and Publication Date\n",
    "- Obtaining journal ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperLoader():\n",
    "    \"\"\"\n",
    "    Initializes PaperLoader class to read all .json files from root_directory\n",
    "    \"\"\"\n",
    "    def __init__(self, root_dir, no_bib=True):\n",
    "        self.ROOT_DIR = root_dir\n",
    "        self.JSON_FILES = glob.glob(f'{root}/**/*.json', recursive=True)\n",
    "        self.PAPERS_COLUMN = {\n",
    "                                \"doc_id\": [None],\n",
    "                                \"title\": [None],\n",
    "                                \"abstract\": [None],\n",
    "                                \"text_body\": [None]\n",
    "                                }\n",
    "        self.PAPERS_DF = None\n",
    "        self.NO_BIB = no_bib\n",
    "    \n",
    "    \"\"\"\n",
    "    Removes sections with more than 5 URL/DOI/HTTP instances\n",
    "    \"\"\"\n",
    "    def __clean_bib(self, body_text):\n",
    "        merged_body = []\n",
    "        for segment in body_text:\n",
    "            if len(merged_body) > 0:\n",
    "                if merged_body[-1]['section'] == segment['section']:\n",
    "                    merged_body[-1]['text'] += '\\n' + segment['text']\n",
    "                    continue\n",
    "            merged_body.append(segment)\n",
    "            \n",
    "        merged_body = [segment for segment in merged_body \n",
    "                       if len(re.findall(\"(http|doi|www)\", segment['text'])) <= 4]\n",
    "        return merged_body\n",
    "\n",
    "        \n",
    "    \"\"\"\n",
    "    Creates a Pandas DataFrame from all json files in root_directory\n",
    "    Each json file represents a paper. \n",
    "    Features extracted are: doc_id, title, abstract, text_body\n",
    "    \"\"\"\n",
    "    def create_paper_df(self):\n",
    "        self.PAPERS_DF = pd.DataFrame.from_dict(self.PAPERS_COLUMN)\n",
    "    \n",
    "        for i in tqdm(range(len(self.JSON_FILES))):\n",
    "            file_name = self.JSON_FILES[i]\n",
    "            row = {x: None for x in self.PAPERS_COLUMN}\n",
    "\n",
    "            with open(file_name) as json_data:\n",
    "                data = json.load(json_data)\n",
    "            \n",
    "                doc_id = data['paper_id']\n",
    "                row['doc_id'] = doc_id\n",
    "                row['title'] = data['metadata']['title']\n",
    "\n",
    "                # Now need all of abstract. Put it all in\n",
    "                # a list then use str.join() to split it\n",
    "                # into paragraphs.\n",
    "\n",
    "                if ('abstract' not in data or 'body_text' not in data):\n",
    "                    continue\n",
    "                else:\n",
    "                    abstract_list = [abst['text'] for abst in data['abstract']]\n",
    "                    abstract = \"\\n \".join(abstract_list)\n",
    "\n",
    "                row['abstract'] = abstract\n",
    "\n",
    "                # And lastly the body of the text.\n",
    "                if self.NO_BIB:\n",
    "                    body_list = self.__clean_bib(data['body_text'])\n",
    "                else:\n",
    "                    body_list = [bt['text'] for bt in data['body_text']]\n",
    "                \n",
    "                \n",
    "                row['text_body'] = body_list\n",
    "\n",
    "\n",
    "                self.PAPERS_DF = self.PAPERS_DF.append(row, ignore_index=True)\n",
    "\n",
    "    \n",
    "        \n",
    "    \"\"\"\n",
    "    Joins paper information with information on journal for paper,\n",
    "    authors, doi and published date\n",
    "    \"\"\"\n",
    "    def merge_metadata(self):\n",
    "        metadata_df = pd.read_csv(self.ROOT_DIR + 'metadata.csv')\n",
    "        metadata_df_for_join = metadata_df.loc[:, \n",
    "                                               ['sha', 'publish_time', 'authors', 'journal', 'doi']]\n",
    "        self.PAPERS_DF = self.PAPERS_DF.merge(metadata_df_for_join, \n",
    "                            left_on='doc_id', right_on='sha', how='inner')\n",
    "\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Joins paper information with information on journal ratings\n",
    "    Important column: H_Index\n",
    "    \"\"\"\n",
    "    def merge_journals(self):\n",
    "        journal_df = pd.read_csv(root + 'scimagoj_2018.csv', sep = ';')\n",
    "        papers_ratings_df = self.PAPERS_DF.merge(journal_df.loc[:,['Title', 'H index']], \n",
    "                           left_on='journal', right_on='Title', how='left')\n",
    "        papers_ratings_df = papers_ratings_df.drop(['sha', 'Title'], \n",
    "                                                   axis=1).reset_index(drop = True)\n",
    "        self.PAPERS_DF = papers_ratings_df\n",
    "\n",
    "    \n",
    "    def get_df(self):\n",
    "        self.PAPERS_DF = self.PAPERS_DF.dropna(subset=['abstract', 'text_body'])\n",
    "        return self.PAPERS_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_loader = PaperLoader(root)\n",
    "paper_loader.create_paper_df()\n",
    "paper_loader.merge_metadata()\n",
    "paper_loader.merge_journals()\n",
    "papers_df = paper_loader.get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers_df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering for covid-19 related papers released after 2019\n",
    "There is a lot of noise in this dataset due to information about other strains of coronavirus so we will select only the papers that are related to Covid-19. \n",
    "\n",
    "\n",
    "Also, while the older papers may contain some important insight on the variance among the  different strains of coronavirus, for our purposes, we will only be looking at papers published on 2019 or later because that is when Covid-19 became popular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `RelevantFilter` class will filter the `DataFrame` from `PaperLoader` and filter for covid-19 papers published on 2019 or later. \n",
    "We will need to supply a list of covid-related keywords to filter from to the `constructor`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelevantFilter():\n",
    "    \"\"\"\n",
    "    Takes a list of key-words that the other methods use\n",
    "    to filter for relevant papers\n",
    "    \"\"\"\n",
    "    def __init__(self, keywords, year = '2019'):\n",
    "        self.KEYWORDS = keywords\n",
    "        self.YEAR = year\n",
    "    \n",
    "    def extract_recent(self, df):\n",
    "        return df[df['publish_time'] >= self.YEAR]\n",
    "    \n",
    "    def filter_papers(self, df):\n",
    "        cov_titles = [title for title in df['title'] \n",
    "                         if any((re.search(key,title.lower())) \n",
    "                         for key in self.KEYWORDS)]\n",
    "        data = df[df['title'].isin(cov_titles)].reset_index(drop = True)\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_list = [\n",
    "    'covid-19','covid 19','novel coronavi',\n",
    "    'cord-19','cord 19','2019-nCoV','cov_2',\n",
    "    '2019 ncov','2019 cov','wuhan coronavi',    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_filter = RelevantFilter(cov_list, '2019')\n",
    "covid_df = covid_filter.filter_papers(papers_df)\n",
    "covid_df = covid_filter.extract_recent(covid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df.head(2)['title'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Design_list = [\n",
    "    'retrospective cohort', 'cross-sectional case-control',\n",
    "    'cross sectional case control', 'prevalence survey', 'systematic review ',\n",
    "    ' meta-analysis', ' meta analysis'\n",
    "    'matched case-control', 'matched case control', 'medical record review',\n",
    "    'observational case series', 'time series analysis',\n",
    "    'pseudo-randomized controlled trials',\n",
    "    'pseudo randomized controlled trial', 'randomized controlled trials',\n",
    "    'randomized controlled trial'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_risk = [\n",
    "    'diabete', 'age', 'neonat', 'elderly,', 'cancer', 'histori', 'sputum',\n",
    "    'stool', 'blood', 'urine', 'house', 'environmental', 'seasonal',\n",
    "    'comorbidit', ' immune deficiency', 'liver', 'smok', 'age decil', 'heart',\n",
    "    'lung', 'climate,', 'PPE use', 'touching face', 'immun', 'insur',\n",
    "    'compromis', 'pregnan', 'race', 'ethnic', 'hyperten', 'child', 'tubercul',\n",
    "    'mtb', 'tb', 'MTB', 'TB'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_outcom = ['risk','range','duration','asymptomatic',\n",
    " 'infecti', 'reproducti',  'route', \n",
    " 'age','transmm'\n",
    " 'stratifi',\n",
    " 'period,',\n",
    " 'health',\n",
    " 'r0','shedd', 'viral'\n",
    " 'period','incub',\n",
    " 'generat',\n",
    " 'factor',\n",
    " 'interval,',\n",
    " 'serial'\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_factors = [\n",
    "                        {\n",
    "                        'name': 'smoking',\n",
    "                        'pattern': 'smok'\n",
    "                        },\n",
    "                        {\n",
    "                        'name': 'diabetes',\n",
    "                        'pattern': 'diabete'\n",
    "                        },\n",
    "                        {\n",
    "                        'name': 'pregnancy',\n",
    "                        'pattern': 'pregnan'\n",
    "                        },\n",
    "                        {\n",
    "                        'name': 'tuberculosis',\n",
    "                        'pattern': '(tubercul|MTB|TB)'\n",
    "                        }\n",
    "                    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_list = [\n",
    "    'mathemat', 'profil', 'cross sectional case control',\n",
    "    'matched case control', 'contact', 'surviv', 'tracing,', 'time to event',\n",
    "    'time-to-event', 'risk factor analysis', 'logistic regression',\n",
    "    'cross-sectional case-control', 'matched case-control',\n",
    "    'observational case series', 'time series analysis', 'survival analysis',\n",
    "    'investigati', 'model', 'outbreak', 'stochast', 'statist', 'analysi',\n",
    "    'experiment', 'excret', 'investig'\n",
    "    'retrospective cohort', 'cross-sectional case-control',\n",
    "    'cross sectional case control', 'prevalence survey', 'systematic review ',\n",
    "    'meta-analysis', 'meta analysis', 'matched case-control',\n",
    "    'matched case control', 'medical record review',\n",
    "    'observational case series', 'time series analysis',\n",
    "    'pseudo-randomized controlled', 'pseudo randomized controlled',\n",
    "    'randomized controlled', 'retrospective analysis', 'retrospective study',\n",
    "    'retrospective studies'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_list = [\n",
    "    'risk', 'range', 'duration', 'asymptomatic', 'infecti', 'reproducti',\n",
    "    'route', 'age', 'transmm'\n",
    "    'stratifi', 'period,', 'health', 'r0', 'shedd', 'viral'\n",
    "    'period', 'incub', 'generat', 'factor', 'interval,', 'serial'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperAnalyzer():\n",
    "    def __init__(self, parent_df):\n",
    "        self.working_df = parent_df\n",
    "        self.__ANALYZED_RISKS = False\n",
    "        self.__RISK_MESSAGE = \"Need to perform risk analyze first. Try calling self.analyze_risks()\"\n",
    "\n",
    "        \n",
    "    def analyze_risks(self, risk_factors):\n",
    "        risk_temp_arr = []    \n",
    "        for index, row in tqdm(self.working_df.iterrows(), \n",
    "                               total=self.working_df.shape[0]):        \n",
    "            for section in row['text_body']:\n",
    "                section_row = row.copy()\n",
    "                section_row['section'] = section['section']\n",
    "                section_row['text_body'] = section['text']\n",
    "                body = section_row['text_body'].lower()\n",
    "                \n",
    "                \n",
    "                for factor in risk_factors:\n",
    "                    name = factor['name']\n",
    "                    matches = re.findall(factor['pattern'], body)\n",
    "                    section_row['has_' + name + '?'] = False\n",
    "                    section_row[name + '_count'] = 0\n",
    "                    section_row[name + '_in_title'] = False\n",
    "                        \n",
    "                    if len(matches) > 0:\n",
    "                        section_row['has_' + name + '?'] = True\n",
    "                        section_row[name + '_count'] = len(matches)\n",
    "                    if re.findall(factor['pattern'], section_row['title']):\n",
    "                        section_row[name + '_in_title'] = True\n",
    "                        # If the risk factor is in title of paper we reward with higher count\n",
    "                        section_row[name + '_count'] += 10\n",
    "            \n",
    "            risk_temp_arr.append(section_row)\n",
    "        \n",
    "        self.working_df = pd.DataFrame(risk_temp_arr)\n",
    "        self.__ANALYZED_RISKS = True\n",
    "\n",
    "    \n",
    "    def analyze_designs(self, design_list):\n",
    "        if not self.__ANALYZED_RISKS:\n",
    "            raise ValueError(self.__ERROR_MESSAGE)\n",
    "        \n",
    "        design_temp_arr = []\n",
    "        for index, row in tqdm(self.working_df.iterrows(), \n",
    "                               total=self.working_df.shape[0]):\n",
    "            design_matches = [re.findall(des, row['text_body'])\n",
    "                                for des in design_list]\n",
    "            design_matches = np.concatenate(design_matches)\n",
    "            design_rank = 0\n",
    "            if len(design_matches) > 0:\n",
    "                if ('meta-analysis' or 'meta analysis') in design_matches:\n",
    "                    design_rank = 6\n",
    "                if ('randomized controlled') in design_matches:\n",
    "                    design_rank = 5\n",
    "                if ('pseudo-randomized controlled'\n",
    "                    or 'pseudo randomized controlled') in design_matches:\n",
    "                    design_rank = 4\n",
    "                design_rank = design_rank + len(design_matches)\n",
    "            row['design_matches'] = design_matches\n",
    "            row['design_rank'] = design_rank\n",
    "            design_temp_arr.append(row)\n",
    "        \n",
    "        self.working_df = pd.DataFrame(design_temp_arr)\n",
    "\n",
    "    \n",
    "    def analyze_outcomes(self, outcomes):\n",
    "        if not self.__ANALYZED_RISKS:\n",
    "            raise ValueError(self.__ERROR_MESSAGE)\n",
    "        \n",
    "        outcome_arr = []\n",
    "        for index, row in tqdm(self.working_df.iterrows(), \n",
    "                               total=self.working_df.shape[0]):\n",
    "            \n",
    "            outcome_matches = [re.findall(outcome, row['text_body'])\n",
    "                                for outcome in outcomes]\n",
    "            outcome_matches = np.concatenate(outcome_matches)\n",
    "            outcome_rank = 0\n",
    "            outcome_rank = len(outcome_matches)\n",
    "            \n",
    "            row['outcome_matches'] = outcome_matches\n",
    "            row['outcome_rank'] = outcome_rank\n",
    "            \n",
    "            outcome_arr.append(row)\n",
    "        \n",
    "        self.working_df = pd.DataFrame(outcome_arr)\n",
    "        \n",
    "    def perform_analysis(self, risk_factors, design_list, outcomes):\n",
    "        print(\"Analyzing risks\")\n",
    "        self.analyze_risks(risk_factors)\n",
    "        print(\"Analyzing study designs\")\n",
    "        self.analyze_designs(design_list)\n",
    "        print(\"Analyzing outcomes\")\n",
    "        self.analyze_outcomes(outcomes)\n",
    "    \n",
    "    def get_df(self, risk_factor = None):\n",
    "        if risk_factor:\n",
    "            if not self.__ANALYZED_RISKS:\n",
    "                raise ValueError(self.__ERROR_MESSAGE)\n",
    "            return self.working_df[self_working_df['has_' + risk_factor + '?'] == True]\n",
    "        return self.working_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fc7c46d224f417186d79b924c87aabb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6495.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "covid_analysis = PaperAnalyzer(covid_df)\n",
    "covid_analysis.analyze_risks(risk_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9545161874874357817a7c341dfbd939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6495.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "covid_analysis.analyze_designs(design_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee3f376489d4c9aa9f82ae01356ea2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6495.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "covid_analysis.analyze_outcomes(outcome_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_covid_df = covid_analysis.get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_covid_df.to_json(\"../../enriched_covid_df.json\", orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('anaconda3': virtualenv)",
   "language": "python",
   "name": "python37464bitanaconda3virtualenv11568a5b709c405b925f37b6b0b6dbdb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
