{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We are a team of three: one machine learning/NLP scientist, one front-end senior software engineer and one co-op data science intern/engineer. We all work in the same place, Thomson Reuters, and that’s how we know each other.  \n",
    "\n",
    "### Task: \n",
    "What do we know about COVID-19 risk factors? \n",
    "\n",
    "### How we approached the challenge: \n",
    "Our platform was built based on the following principles: \n",
    "- Empower users (health researchers) to conduct literature survey efficiently. \n",
    "- Adaptable to future needs and challenges of health researchers.  \n",
    "- Modularized to have the capability of being improved and polished in a short time and in parallel.  \n",
    "\n",
    "### Our hypotheses/why? \n",
    "\n",
    "AI can benefit researchers by extracting and visualizing information in the most efficient and relevant manner at scale.  In the absence of expert feedback and annotations, we have built/provided a platform that extracts and ranks relevant info with potential for improvement in the future by taking advantage of expert annotation.\n",
    "\n",
    "### How we solve it/why? \n",
    "\n",
    "The project was done in two slightly different approaches.  \n",
    "The first approach focused on developing an end to end pipeline to address the first subtask which was: \n",
    "- Data on potential risks factors \n",
    "- Smoking, pre-existing pulmonary disease \n",
    "- Co-infections (determine whether co-existing respiratory/viral infections make the virus more transmissible or virulent) and other co-morbidities \n",
    "- Neonates and pregnant women \n",
    "- Socio-economic and behavioral factors to understand the economic impact of the virus and whether there were differences. \n",
    "\n",
    "We have developed a web app to visually accompany the analysis carried out for this task. \n",
    "The second approach was focused on question information retrieval but isn’t accompanied by with a web-app yet due to time constraint. \n",
    "\n",
    "### First approach had the following recipe: \n",
    "The platform is designed to visualize snippets of relevant topics through the following process: \n",
    "- Merge different sources of the data\n",
    "- Extract all COVID 19 related papers \n",
    "- Expand and process the list of keywords\n",
    "- Find excerpts of papers that include the keywords \n",
    "- Rank the excerpts\n",
    "- Create an extractive summary for each paper\n",
    "- Serve it to the web-app for visualization\n",
    "\n",
    "### Second approach had the following recipe: \n",
    "The platform is designed to rank snippets of relevant topics through the following process: \n",
    "\n",
    "- Merge different sources of the data\n",
    "- Extract all COVID 19 related paper s\n",
    "- Convert a question into set of keywords (The questions are extracted from the [medical dictionary](https://docs.google.com/spreadsheets/d/1NoiAFJoydk3zuc-G0qqROarkhaGpfgbQhTVYhbYtLCM/edit#gid=0) shared with Kaggle participants.\n",
    "- Expand the list of keywords\n",
    "- Find excerpts of papers that include the keywords \n",
    "- Rank the excerpts\n",
    "- Create an extractive summary for each paper \n",
    "\n",
    "### Pros and Cons of our approach and platform: \n",
    "\n",
    "#### Pros: \n",
    "\n",
    "- The code base is highly modular and simple  \n",
    "- The code is very well documented \n",
    "- The visualization resonates with researchers. We interviewed an expert in epidemiology, since that is our target user. He liked all aspects of it including the extractive summaries of the papers. He also noted that using this platform could reduce the time taken for a literature survey, which normally takes 3 to 4 months, to less than a month.  \n",
    "- The platform is taking a high recall approach to include everything \n",
    "- With a little bit of effort, we can convert this platform to an expert annotation platform where experts can interactively click on irrelevant sentences in the snippets and convert a completely unsupervised approach to a supervised learning task. \n",
    "\n",
    "#### Cons: \n",
    "- Keyword search is not the most efficient search. Word embedding is known to be a better approach for text analysis but there wasn’t enough time to implement and evaluate that. \n",
    "- Our ranking follows a simple approach, where an expert should decide on the metric for ranking the importance of a snippet. \n",
    "- The second approach is not coming with a visualization. \n",
    "- There may be more risk factors than what we currently searched for and an algorithm should extract the unknown risks too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "import sys\n",
    "import glob\n",
    "import json\n",
    "import string\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "root = '../../kaggle_data/'\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "stop_words.extend(['within', 'what', 'how', 'eg', 'ie'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Parsing and Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `PaperLoader` class will load all papers for the challenge and provide an interface for us to obtain `DataFrames` to work with. The focus will be on:\n",
    "- Obtaining Paper title, Abstract, Body\n",
    "    - The text body is filtered to remove sections containing lots of citations and hyperlinks\n",
    "- Obtaining Authors, Journal of Publication, Publication Date and Publication Date\n",
    "- Obtaining journal ratings(H index) to potentially sort paper based on journal quality\n",
    "    - For the journal ratings, we use a list we obtained from **INSERT LINK HERE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperLoader():\n",
    "    \"\"\"\n",
    "    Loads, parses and merges metadata for papers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, no_bib=True):\n",
    "        \"\"\"\n",
    "        Initializes PaperLoader class to read all .json files from root_directory\n",
    "            \n",
    "            no_bib: if true, clean noisy sections with bibliographies\n",
    "            root_dir: root directory for papers\n",
    "        \"\"\"\n",
    "        self.ROOT_DIR = root_dir\n",
    "        self.JSON_FILES = glob.glob(f'{root}/**/*.json', recursive=True)\n",
    "        self.PAPERS_COLUMN = {\n",
    "            \"doc_id\": [None],\n",
    "            \"title\": [None],\n",
    "            \"abstract\": [None],\n",
    "            \"text_body\": [None]\n",
    "        }\n",
    "        self.PAPERS_DF = None\n",
    "        self.NO_BIB = no_bib\n",
    "\n",
    "    \n",
    "    def __clean_bib(self, body_text, thres):\n",
    "        \"\"\"\n",
    "        Removes sections with more than 5 URL/DOI/HTTP instances\n",
    "            \n",
    "            body_text: array of dictionaries for text_body\n",
    "            thres: number of hyperlinks tolerated before removal \n",
    "        \"\"\"\n",
    "        # Sometimes, the text body has duplicate sections consecutively.\n",
    "        merged_body = []\n",
    "        for segment in body_text:\n",
    "            # We will combine these duplicate sections\n",
    "            if len(merged_body) > 0:\n",
    "                if merged_body[-1]['section'] == segment['section']:\n",
    "                    merged_body[-1]['text'] += '\\n' + segment['text']\n",
    "                    continue\n",
    "            merged_body.append(segment)\n",
    "\n",
    "        merged_body = [\n",
    "            segment for segment in merged_body\n",
    "            if len(re.findall(\"(http|doi|www)\", segment['text'])) <= thres\n",
    "        ]\n",
    "        return merged_body\n",
    "\n",
    "\n",
    "    def create_paper_df(self):\n",
    "        \"\"\"\n",
    "        Creates a Pandas DataFrame from all json files in root_directory\n",
    "        Each json file represents a paper. \n",
    "        Features extracted are: doc_id, title, abstract, text_body\n",
    "        \"\"\"\n",
    "        df_list = []\n",
    "        \n",
    "        for i in tqdm(range(len(self.JSON_FILES))):\n",
    "            file_name = self.JSON_FILES[i]\n",
    "            \n",
    "            #Initialize row for returned df. Each row represents a paper\n",
    "            row = {x: None for x in self.PAPERS_COLUMN}\n",
    "\n",
    "            with open(file_name) as json_data:\n",
    "                data = json.load(json_data)\n",
    "\n",
    "                row['doc_id'] = data['paper_id']\n",
    "                row['title'] = data['metadata']['title']\n",
    "                \n",
    "                # If title is empty, we skip the paper\n",
    "                if len(row['title']) <= 2:\n",
    "                    continue\n",
    "\n",
    "                # If a paper does not have an abstract of a body, we will skip it\n",
    "                if ('abstract' not in data or 'body_text' not in data):\n",
    "                    continue\n",
    "                else:\n",
    "                    # Now need all of the abstract. Put it all in\n",
    "                    # a list then use str.join() \n",
    "                    abstract_list = [abst['text'] for abst in data['abstract']]\n",
    "                    abstract = \"\\n \".join(abstract_list)\n",
    "\n",
    "                # Skip the paper if abstract is empty\n",
    "                if len(abstract) <= 2:\n",
    "                    continue\n",
    "\n",
    "                row['abstract'] = abstract\n",
    "\n",
    "                # And lastly the body of the text.\n",
    "                # These clauses check if the user wants to clean up references\n",
    "                if self.NO_BIB:\n",
    "                    body_list = self.__clean_bib(data['body_text'], 4)\n",
    "                else:\n",
    "                    body_list = [bt for bt in data['body_text']]\n",
    "\n",
    "                row['text_body'] = body_list\n",
    "\n",
    "                df_list.append(row)\n",
    "        # create final dataframe\n",
    "        self.PAPERS_DF = pd.DataFrame(df_list)\n",
    "\n",
    "\n",
    "    def merge_metadata(self, metadata = 'metadata.csv'):\n",
    "        \"\"\"\n",
    "            Joins paper information with information on journal for paper,\n",
    "            authors, doi and published date  \n",
    "                metadata: path to csv file containing metadata\n",
    "        \"\"\"\n",
    "        metadata_df = pd.read_csv(self.ROOT_DIR + metadata)\n",
    "        metadata_df = metadata_df.loc[:, \n",
    "                          ['sha', 'publish_time', 'authors', 'journal', 'doi']]\n",
    "        self.PAPERS_DF = self.PAPERS_DF.merge(metadata_df,\n",
    "                                              left_on='doc_id',\n",
    "                                              right_on='sha',\n",
    "                                              how='inner')\n",
    "\n",
    "    def merge_journals(self):\n",
    "        \"\"\"\n",
    "        Joins paper information with information on journal ratings\n",
    "        Important column: H_Index\n",
    "        \"\"\"\n",
    "        journal_df = pd.read_csv(root + 'scimagoj_2018.csv', sep=';')\n",
    "        papers_ratings_df = self.PAPERS_DF.merge(\n",
    "            journal_df.loc[:, ['Title', 'H index']],\n",
    "            left_on='journal',\n",
    "            right_on='Title',\n",
    "            how='left')\n",
    "        papers_ratings_df = papers_ratings_df.drop(\n",
    "            ['sha', 'Title'], axis=1).reset_index(drop=True)\n",
    "        self.PAPERS_DF = papers_ratings_df\n",
    "\n",
    "    def get_df(self):\n",
    "        \"\"\"\n",
    "        Returns processed dataframe\n",
    "        \"\"\"\n",
    "        self.PAPERS_DF = self.PAPERS_DF.dropna(\n",
    "            subset=['abstract', 'text_body'])\n",
    "        return self.PAPERS_DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now parse the papers from our data(root) directory and store them in `papers_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83876000e208411ca7b3386f6b52a198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=59311.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "paper_loader = PaperLoader(root)\n",
    "paper_loader.create_paper_df()\n",
    "paper_loader.merge_metadata()\n",
    "paper_loader.merge_journals()\n",
    "papers_df = paper_loader.get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>text_body</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>doi</th>\n",
       "      <th>H index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>306ef95a3a91e13a93bcc37fb2c509b67c0b5640</td>\n",
       "      <td>A Novel Approach for a Novel Pathogen: using a...</td>\n",
       "      <td>Thousands of people in the United States have ...</td>\n",
       "      <td>[{'text': 'The 2019 novel coronavirus (SARS-Co...</td>\n",
       "      <td>2020-03-12</td>\n",
       "      <td>Bryson-Cahn, Chloe; Duchin, Jeffrey; Makarewic...</td>\n",
       "      <td>Clin Infect Dis</td>\n",
       "      <td>10.1093/cid/ciaa256</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6599ebbef3d868afac9daa4f80fa075675cf03bc</td>\n",
       "      <td>International aviation emissions to 2025: Can ...</td>\n",
       "      <td>International aviation is growing rapidly, res...</td>\n",
       "      <td>[{'text': 'Sixty years ago, civil aviation was...</td>\n",
       "      <td>2009-01-31</td>\n",
       "      <td>Macintosh, Andrew; Wallace, Lailey</td>\n",
       "      <td>Energy Policy</td>\n",
       "      <td>10.1016/j.enpol.2008.08.029</td>\n",
       "      <td>178.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     doc_id  \\\n",
       "0  306ef95a3a91e13a93bcc37fb2c509b67c0b5640   \n",
       "1  6599ebbef3d868afac9daa4f80fa075675cf03bc   \n",
       "\n",
       "                                               title  \\\n",
       "0  A Novel Approach for a Novel Pathogen: using a...   \n",
       "1  International aviation emissions to 2025: Can ...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Thousands of people in the United States have ...   \n",
       "1  International aviation is growing rapidly, res...   \n",
       "\n",
       "                                           text_body publish_time  \\\n",
       "0  [{'text': 'The 2019 novel coronavirus (SARS-Co...   2020-03-12   \n",
       "1  [{'text': 'Sixty years ago, civil aviation was...   2009-01-31   \n",
       "\n",
       "                                             authors          journal  \\\n",
       "0  Bryson-Cahn, Chloe; Duchin, Jeffrey; Makarewic...  Clin Infect Dis   \n",
       "1                 Macintosh, Andrew; Wallace, Lailey    Energy Policy   \n",
       "\n",
       "                           doi  H index  \n",
       "0          10.1093/cid/ciaa256      NaN  \n",
       "1  10.1016/j.enpol.2008.08.029    178.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25312, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering for covid-19 related papers released after 2019\n",
    "There is a lot of noise in this dataset due to information about other strains of coronavirus so we will select only the papers that are related to Covid-19. \n",
    "\n",
    "While the older papers may contain some important insight on the variance among the  different strains of coronavirus, for our purposes, we will only be looking at papers published on 2019 or later because that is when Covid-19 was first discovered in humans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of keywords for covid-19\n",
    "cov_list = [\n",
    "    'novel coronavi',\n",
    "    'covid',\n",
    "    'cov_2',\n",
    "    'cord-19',\n",
    "    'cord 19',\n",
    "    '2019-nCoV',\n",
    "    '2019 ncov',\n",
    "    '2019 cov',\n",
    "    'wuhan coronavi',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `RelevantFilter` class will filter the dataframe from `PaperLoader` and filter for covid-19 papers published on 2019 or later. \n",
    "We will need to supply a list of covid-related keywords to filter from to the `constructor`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RelevantFilter():\n",
    "    \n",
    "    def __init__(self, keywords, year='2019'):\n",
    "        \"\"\"\n",
    "        constructor for RelevantFilter\n",
    "            keywords: keywords to filter for\n",
    "            year: papers written before this year will be discarded\n",
    "        \"\"\"\n",
    "        self.KEYWORDS = keywords\n",
    "        self.YEAR = year\n",
    "\n",
    "    def extract_recent(self, df):\n",
    "        \"\"\"\n",
    "        extracts documents published on or after self.YEAR\n",
    "        \"\"\"\n",
    "        return df[df['publish_time'] >= self.YEAR]\n",
    "\n",
    "    def filter_papers(self, df):\n",
    "        \"\"\"\n",
    "        Filters for papers whose title have mention of \n",
    "        any of the terms in self.KEYWORDS\n",
    "        \"\"\"\n",
    "        pattern = re.compile('(' + \"|\".join(self.KEYWORDS) + ')',\n",
    "                                 re.IGNORECASE)\n",
    "        # We will filter for rows with one or more matches \n",
    "        # for title and covid keywords\n",
    "        df = df[df['title'].apply(lambda x: \n",
    "                                  len(pattern.findall(x)) >= 1\n",
    "                                  if x else False)]\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will filter through `papers_df` to get only covid-19 related papers in `covid_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_filter = RelevantFilter(cov_list, '2019')\n",
    "covid_df = covid_filter.filter_papers(papers_df)\n",
    "covid_df = covid_filter.extract_recent(covid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(929, 9)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>text_body</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>doi</th>\n",
       "      <th>H index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>306ef95a3a91e13a93bcc37fb2c509b67c0b5640</td>\n",
       "      <td>A Novel Approach for a Novel Pathogen: using a...</td>\n",
       "      <td>Thousands of people in the United States have ...</td>\n",
       "      <td>[{'text': 'The 2019 novel coronavirus (SARS-Co...</td>\n",
       "      <td>2020-03-12</td>\n",
       "      <td>Bryson-Cahn, Chloe; Duchin, Jeffrey; Makarewic...</td>\n",
       "      <td>Clin Infect Dis</td>\n",
       "      <td>10.1093/cid/ciaa256</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     doc_id  \\\n",
       "0  306ef95a3a91e13a93bcc37fb2c509b67c0b5640   \n",
       "\n",
       "                                               title  \\\n",
       "0  A Novel Approach for a Novel Pathogen: using a...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Thousands of people in the United States have ...   \n",
       "\n",
       "                                           text_body publish_time  \\\n",
       "0  [{'text': 'The 2019 novel coronavirus (SARS-Co...   2020-03-12   \n",
       "\n",
       "                                             authors          journal  \\\n",
       "0  Bryson-Cahn, Chloe; Duchin, Jeffrey; Makarewic...  Clin Infect Dis   \n",
       "\n",
       "                   doi  H index  \n",
       "0  10.1093/cid/ciaa256      NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A Novel Approach for a Novel Pathogen: using a home assessment team to evaluate patients for 2019 novel coronavirus (SARS-CoV-2)',\n",
       " 'Modeling the dynamics of novel coronavirus (2019-nCov) with fractional derivative']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(covid_df.head(2)['title'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Analysis (Phase 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now go through the papers to extract and rank excerpts that contain relevant information about risk factors for covid-19. \n",
    "We will do this through an analysis of:\n",
    "\n",
    "- Risk factors for covid-19\n",
    "- Study designs\n",
    "    - We will use this to evaluate the quality of a paper's methodologies for our rankings\n",
    "- Outcomes\n",
    "    - We will incentivise excerpts to explicitly mention outcomes that we have found researchers look for(in our interviews)\n",
    "- Fatality\n",
    "    - We have determined that information on mortality and fatality would be of high value to researcehrs, and rightly so.\n",
    "\n",
    "**Note**: The list of keywords were all obtained from a crowdsourced medical dictionary researchers had assembled. You can find more details [here](https://docs.google.com/spreadsheets/d/1t2e3CHGxHJBiFgHeW0dfwtvCG4x0CDCzcTFX7yz9Z2E/edit#gid=1217643351)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_factors = [{\n",
    "    'name': 'smoking',\n",
    "    'pattern': 'smok'\n",
    "}, {\n",
    "    'name': 'diabetes',\n",
    "    'pattern': 'diabete'\n",
    "}, {\n",
    "    'name': 'pregnancy',\n",
    "    'pattern': 'pregnan'\n",
    "}, {\n",
    "    'name': 'tuberculosis',\n",
    "    'pattern': '(tubercul|mtb|\\btb[A-Za-z0-9]\\b)'\n",
    "}, {\n",
    "    'name': 'hypertension',\n",
    "    'pattern': 'hypertension'\n",
    "}, {\n",
    "    'name': 'cancer',\n",
    "    'pattern': 'cancer'\n",
    "}, {\n",
    "    'name': 'neonates',\n",
    "    'pattern': '(baby|neonate|enfant)'\n",
    "},\n",
    "    {\n",
    "    'name': 'liver disease',\n",
    "    'pattern': 'liver disease'\n",
    "},{\n",
    "    'name': 'COPD',\n",
    "    'pattern': 'COPD'\n",
    "},{\n",
    "    'name': 'pulmonary disease',\n",
    "    'pattern': 'pulm'\n",
    "},{\n",
    "    'name': 'race/ethnicity',\n",
    "    'pattern': 'ethnic'\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "design_list = [\n",
    "    'mathemat', 'profil', 'cross sectional case control',\n",
    "    'matched case control', 'contact', 'surviv', 'tracing,', 'time to event',\n",
    "    'time-to-event', 'risk factor analysis', 'logistic regression',\n",
    "    'cross-sectional case-control', 'matched case-control',\n",
    "    'observational case series', 'time series analysis', 'survival analysis',\n",
    "    'investigati', 'model', 'outbreak', 'stochast', 'statist', 'analysi',\n",
    "    'experiment', 'excret', 'investig',\n",
    "    'retrospective cohort', 'cross-sectional case-control',\n",
    "    'cross sectional case control', 'prevalence survey', 'systematic review ',\n",
    "    'meta-analysis', 'meta analysis', 'matched case-control',\n",
    "    'matched case control', 'medical record review',\n",
    "    'observational case series', 'time series analysis',\n",
    "    'pseudo-randomized controlled', 'pseudo randomized controlled',\n",
    "    'randomized controlled', 'retrospective analysis', 'retrospective study',\n",
    "    'retrospective studies'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_list = [\n",
    "    'risk', 'range', 'duration', 'asymptomatic', 'infecti', 'reproducti',\n",
    "    'route', 'age', 'transmm'\n",
    "    'stratifi', 'period,', 'health', 'r0', 'shedd', 'viral'\n",
    "    'period', 'incub', 'generat', 'factor', 'interval,', 'serial'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "fatality_list = ['icu', 'fatal', 'death', 'die', 'dead', 'dying', 'mortal']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The next cell will contain the default coeffecients for the algorithm's prioritization of different features. These coeffecients are a work in progress and we seek to constantly improve them with more expert feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_weights= {\n",
    "    'risk': 2,\n",
    "    'design': 1,\n",
    "    'outcome': 2,\n",
    "    'fatality': 1,\n",
    "    'section': 1,\n",
    "    'inverse_length': 5\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `PaperAnalyzer` class will take in a DataFrame of papers and then analyze each paper. \n",
    "The analysis is done with with its `analyze_risks()`, `analyze_designs()` and `analyze_outcomes()` methods that will analyze the risk factors, designs and outcomes respectively for excerpts in the paper. Finally, the `get_df()` method will return a new DataFrame with rankings for relevancy of excerpts. The rankings also factor in the `section` of the paper that the excerpt is from, with sections like **discussion** or **results** that seem to have pertinent, concise information ranked higher. Furthermore, these rankings are also normalized by the lenght of the excerpts\n",
    "\n",
    "*Note: These rankings for sections were determined through our interviews with epidimiologists.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions for `PaperAnalyzer` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_design(design_keyword):\n",
    "    \"\"\"\n",
    "    This helper function ranks study designs. So far we have\n",
    "    confirmed rankings for only three study designs, but this\n",
    "    data will be expanded and improved further with time as we \n",
    "    speak to more epidimiologists\n",
    "    \"\"\"\n",
    "    design_rankings = {\n",
    "        'meta': 10,\n",
    "        'random': 8,\n",
    "        'pseudo': 6,\n",
    "    }\n",
    "    current_ranking = -1\n",
    "    for key in design_rankings.keys():\n",
    "        if key in design_keyword.lower():\n",
    "            current_ranking = min(current_ranking, design_rankings[key])\n",
    "    \n",
    "    if current_ranking == -1:\n",
    "        current_ranking = 4\n",
    "    return current_ranking    \n",
    "\n",
    "def flatten(arr):\n",
    "    \"\"\"\n",
    "    Returns a single flat list from a list of lists\n",
    "    \"\"\"\n",
    "    return [item \n",
    "            for sublist in arr \n",
    "            for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperAnalyzer():    \n",
    "    \"\"\"\n",
    "    Takes in a dataframe of papers and sets it up for analysis\n",
    "    \"\"\"\n",
    "    # Setting up static constants\n",
    "    DEFAULT_RISKS = risk_factors\n",
    "    DEFAULT_DESIGNS = design_list\n",
    "    DEFAULT_OUTCOMES = outcome_list\n",
    "    DEFAULT_FATAL = fatality_list\n",
    "    DEFAULT_WEIGHTS = evaluation_weights\n",
    "    \n",
    "    def __init__(self, parent_df, weights = None):\n",
    "        \"\"\"\n",
    "        Explodes the passed dataframe on sections for more granular analysis\n",
    "        Sets up ranks to be updated later by methods. Client can supply their\n",
    "        own dictionary of weights for different features.\n",
    "        \"\"\"\n",
    "        # Section ratings\n",
    "        self.section_ratings = {\n",
    "                        'discus': 10,\n",
    "                        'concl': 10,\n",
    "                        'resul': 10,\n",
    "                        'analy': 9,\n",
    "                        'impli': 9,\n",
    "                        'valu': 9,\n",
    "                        'intro': 6\n",
    "                        }\n",
    "        \n",
    "        parent_df['full_text'] = parent_df['text_body'].apply(lambda x: '\\n'.join([sec['text'] for sec in x]))\n",
    "        self.df = parent_df.explode('text_body')\n",
    "        # Extracting section headers\n",
    "        self.df['section'] = self.df['text_body'].apply(lambda x: \n",
    "                                                        x['section'] \n",
    "                                                        if type(x) == dict \n",
    "                                                        else None)\n",
    "        # Extracting section texts\n",
    "        self.df['text_body'] = self.df['text_body'].apply(lambda x:\n",
    "                                                          x['text'] \n",
    "                                                          if type(x) == dict \n",
    "                                                          else None)\n",
    "        # Dropping rows where section text is empty\n",
    "        self.df = self.df[self.df['text_body'].notna()]\n",
    "        self.df['total_rank'] = 0\n",
    "        if weights:\n",
    "            self.weights = weights\n",
    "        else:\n",
    "            self.weights = PaperAnalyzer.DEFAULT_WEIGHTS\n",
    "        # TQDM is used for progress bars\n",
    "        tqdm.pandas()\n",
    "\n",
    "    def analyze_risks(self, risk_factors):\n",
    "        \"\"\"\n",
    "        Analyses papers in self.df for risk factors and returns a report df\n",
    "        with columns has_{risk_factor}?, {risk_factor}_count, \n",
    "        {risk_factor}_in_title and updates {total_rank} for each row.\n",
    "        The match_indices column is produced for ease of visualization\n",
    "        in the web app.\n",
    "        \"\"\"\n",
    "        if risk_factors == None:\n",
    "            risk_factors = PaperAnalyzer.DEFAULT_RISKS\n",
    "        \n",
    "        if type(risk_factors[0]) == dict:\n",
    "            patterns = [risk['pattern'] for risk in risk_factors]\n",
    "        elif type(risk_factors[0]) == str:\n",
    "            patterns = [risk for risk in risk_factors]\n",
    "        \n",
    "        self.df = self.df[self.df['text_body'].apply(lambda x:\n",
    "                                                    any(re.compile(pattern, re.IGNORECASE).findall(x)\n",
    "                                                       for pattern in patterns)\n",
    "                                                    )]\n",
    "        self.df['risk_factors'] = [[]] * len(self.df)\n",
    "        self.df['match_indices'] = [[]] * len(self.df)\n",
    "        for i in tqdm(range(len(risk_factors))):\n",
    "            factor = risk_factors[i]\n",
    "            if type(factor) == dict:\n",
    "                name = factor['name']\n",
    "                pattern = re.compile(factor['pattern'], re.IGNORECASE)\n",
    "            elif type(factor) == str:\n",
    "                name = factor\n",
    "                pattern = re.compile(factor, re.IGNORECASE)\n",
    "    \n",
    "            self.df['_matches'] = self.df['text_body'].apply(lambda x: \n",
    "                                                                      [(m.start(), m.group()) \n",
    "                                                                       for m in pattern.finditer(x)])\n",
    "            \n",
    "            self.df[name + '_count'] = self.df['_matches'].apply(lambda x: len(x))\n",
    "            self.df['has_' + name + '?'] = self.df[name + '_count'].apply(lambda x: x > 0)\n",
    "            self.df[name + '_in_title'] = self.df['title'].apply(lambda x:\n",
    "                                                                         len(pattern.findall(x)) > 0)\n",
    "            self.df[name + '_count'] = self.df.apply(lambda x:\n",
    "                                                             x[name + '_count'] + 10\n",
    "                                                            if x[name + '_in_title'] \n",
    "                                                             else x[name + '_count'],\n",
    "                                                            axis=1)\n",
    "            self.df['total_rank'] += self.weights['risk'] * self.df[name + '_count']\n",
    "            \n",
    "            self.df['risk_factors'] = self.df.apply(lambda x: \n",
    "                                    x['risk_factors'] + [name] if x['has_' + name + '?']\n",
    "                                    else x['risk_factors'],\n",
    "                                   axis=1)\n",
    "            self.df['match_indices'] = self.df.apply(lambda x: \n",
    "                                    x['match_indices'] + [n[0] for n in x['_matches']] if x['has_' + name + '?']\n",
    "                                    else x['match_indices'],\n",
    "                                   axis=1)\n",
    "            self.df.drop('_matches', axis=1, inplace=True)\n",
    "            \n",
    "\n",
    "    def analyze_designs(self, design_list):\n",
    "        \"\"\"\n",
    "        Analyses papers in self.df for study designs and returns a report df \n",
    "        with 'design' and 'design_rank'. 'design_rank' is decided upon from the \n",
    "        input in crowdsourced medical dictionary.\n",
    "        \"\"\"\n",
    "        if design_list == None:\n",
    "            design_list = PaperAnalyzer.DEFAULT_DESIGNS\n",
    "        self.df['design'] = self.df['text_body'].progress_apply(lambda x:\n",
    "                                                                      [re.findall(des, x, re.IGNORECASE) \n",
    "                                                                       for des in design_list])\n",
    "        self.df['design_rank'] = self.df['design'].apply(lambda x:\n",
    "                                                                len(x))\n",
    "        self.df['design_rank'] += self.df['design'].apply(lambda x: rank_design(' '.join(flatten(x))))\n",
    "        self.df['total_rank'] += self.weights['design'] * self.df['design_rank']\n",
    "\n",
    "    def analyze_outcomes(self, outcomes):\n",
    "        \"\"\"\n",
    "        Analyses papers in self.df for outcomes and returns a report df \n",
    "        with 'outcomes' and 'outcome_rank'. 'outcome_rank' is decided upon \n",
    "        by the frequency of mentions of outcomes in the excerpt\n",
    "        \"\"\"\n",
    "        \n",
    "        if outcomes == None:\n",
    "            outcomes = PaperAnalyzer.DEFAULT_OUTCOMES\n",
    "        self.df['outcomes'] = self.df['text_body'].progress_apply(lambda x:\n",
    "                                                                        [re.findall(outcome, x, re.IGNORECASE)\n",
    "                                                                         for outcome in outcomes])\n",
    "        self.df['outcome_rank'] = self.df['outcomes'].apply(lambda x: len(x))\n",
    "        self.df['total_rank'] += self.weights['outcome']* self.df['outcome_rank']\n",
    "        \n",
    "    def analyze_fatality(self, fatality_list):\n",
    "        \"\"\"\n",
    "        Analyses papers in self.df for information on fatality \n",
    "        returns a report df with 'fatality_rank'. \n",
    "        'fatality_rank' is decided upon by the frequency of \n",
    "        mentions of fatality in the excerpt\n",
    "        \"\"\"\n",
    "        \n",
    "        if fatality_list == None:\n",
    "            fatality_list = PaperAnalyzer.DEFAULT_FATAL\n",
    "        self.df['fatality_count'] = self.df['text_body'].progress_apply(lambda x:\n",
    "                                                                        len([re.findall(key, x, re.IGNORECASE)\n",
    "                                                                         for key in fatality_list]))\n",
    "        self.df['fatal_info?'] = self.df['fatality_count'].apply(lambda x: x > 0)\n",
    "        self.df['total_rank'] += self.weights['fatality']* self.df['fatality_count']\n",
    "\n",
    "    def perform_analysis(self, risk_factors, design_list=None, outcomes=None, fatality_list = None):\n",
    "        \"\"\"\n",
    "        This function is a wrapper function that provides interface\n",
    "        to conduct analysis on all of risk factors, study designs and\n",
    "        outcomes. Users may specify their own design_list or outcomes. If not,\n",
    "        the default is used.\n",
    "        \"\"\"\n",
    "        print(\"Analyzing risks\")\n",
    "        self.analyze_risks(risk_factors)\n",
    "        print(\"Analyzing study designs\")\n",
    "        self.analyze_designs(design_list)\n",
    "        print(\"Analyzing outcomes\")\n",
    "        self.analyze_outcomes(outcomes)\n",
    "        print(\"Analyzing fatality\")\n",
    "        self.analyze_fatality(fatality_list)\n",
    "        print(\"Generating final rankings\")\n",
    "        self.generate_risk_rankings()\n",
    "\n",
    "        \n",
    "    def generate_risk_rankings(self):\n",
    "        \"\"\"\n",
    "        Appends columns in self.df that contain individual rankings for \n",
    "        each risk factor\n",
    "        \"\"\"\n",
    "        self.df['section_rank'] = self.df['section'].apply(lambda x: self.section_ratings[x] \n",
    "                                                           if x in self.section_ratings else 5)\n",
    "        # Obtaining list of risk factors\n",
    "        risk_factors = [column for column in self.df.columns if 'has_' in column]\n",
    "        risk_factors = [factor[4:-1] for factor in risk_factors]\n",
    "        for i in tqdm(range(len(risk_factors))):\n",
    "            factor = risk_factors[i]\n",
    "            self.df[factor + '_rank'] = (\n",
    "                            self.weights['risk']*self.df[factor + '_count'] + \n",
    "                            self.weights['section']*self.df['section_rank'] + \n",
    "                            self.weights['design']*self.df['design_rank'] + \n",
    "                            self.weights['outcome']*self.df['outcome_rank'] + \n",
    "                            self.weights['fatality']* self.df['fatality_count']\n",
    "            )\n",
    "            \n",
    "            # Normalizing risk rank for length of excerpts\n",
    "            self.df[factor + '_rank'] = self.df.apply(lambda x: x[factor + '_rank'] + \n",
    "                                                        (self.weights['inverse_length']/\n",
    "                                                         (len(word_tokenize(x['text_body'])))), \n",
    "                                              axis=1)\n",
    "        \n",
    "        self.df['max_rank'] = self.df[[factor + '_rank' for factor in risk_factors]].max(axis=1)\n",
    "        self.df['total_rank'] += self.weights['section'] * self.df['section_rank']\n",
    "        # Normalizing total rank for length of excerpts\n",
    "        self.df['total_rank'] = self.df.apply(lambda x: x['total_rank'] + \n",
    "                                                        (self.weights['inverse_length']/\n",
    "                                                         (len(word_tokenize(x['text_body'])))), \n",
    "                                              axis=1)\n",
    "    \n",
    "    def get_df(self, risk_factor=None):\n",
    "        \"\"\"\n",
    "        Applies section ratings, updates total ratings and returns reporting df\n",
    "            risk_factor: if specified, the returned df will only have excerpts\n",
    "                            that mention this risk factor\n",
    "        \"\"\"\n",
    "        if risk_factor:\n",
    "            if not self.__ANALYZED_RISKS:\n",
    "                raise ValueError(self.__ERROR_MESSAGE)\n",
    "            return self.df[self.df['has_' + risk_factor + '?'] == True]\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/tqdm/std.py:658: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f86b12cda841e58217422a193e4d7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a25806d89da4505b7bd73b23b257be8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=652.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb4bd1a94ebf4c4f9f0ab5cc28686c06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=652.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42d4ec3779f146578eee7ecd90a5fe1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=652.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05253452738d4d608a4137868c0bdee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=11.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "covid_analysis = PaperAnalyzer(covid_df)\n",
    "covid_analysis.analyze_risks(risk_factors)\n",
    "covid_analysis.analyze_designs(design_list)\n",
    "covid_analysis.analyze_outcomes(outcome_list)\n",
    "covid_analysis.analyze_fatality(fatality_list)\n",
    "covid_analysis.generate_risk_rankings()\n",
    "enriched_covid_df = covid_analysis.get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(652, 66)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enriched_covid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"We found a significant higher ACE2 gene expression in smoker (including current smoker and former smoker) samples compared to non-smoker samples in the TCGA (p-value=0.05) and GSE40419 RNA-seq datasets (p-value=0.01, Fig. 2A ). Smokers in GSE10072 showed a higher mean of ACE2 gene expression than non-smokers. The difference is not significant (p-value=0.18), which may be due to the small sample size of this study (n=33) with insufficient power to detect the difference. The GSE19804 data which has only non-smoker samples available was not included into the analysis. Adjusted by other factors (age, gender, race and platforms) in multivariate analysis, smoking still shows a significant disparity in ACE2 gene expression (p-value=0.01, Fig. 1B) . These data were from the normal lung tissue of patients with lung adenocarcinoma, which may be different with the lung tissue of healthy people. Therefore, we also analyzed a gene expression dataset of airway epithelium from healthy smokers and healthy non-smokers. Consistently, we observed a significant upregulation of ACE2 gene expression in both large airway epithelium and small airway epithelium of smokers (p-value=4.99E-4 and 7.02E-3, respectively, Fig. 2C ). Furthermore, we investigated ordinal categorized smoking history (non-smoker, former smoker quit more than 15yrs, former smoker quit less than15yrs, and current smoker) and showed results in Figure 2B . In the TCGA dataset, we found a significant trend of ACE2 gene expression regulation associated with the smoking history that current smokers had the highest expression, non-smokers had the lowest expression and former smokers had that inbetween (ordinal regression p-value=0.01, Fig. 2B ). We found a similar but non-significant trend in the GSE10072 due to its small size (ordinal regression p-value=0.11). We didn't observe such a tread in the TCGA dataset. Instead, we found a higher average expression in recent quitters (<=15 years) compared to non-smokers, current smokers and former . CC-BY-NC-ND 4.0 International license It is made available under a author/funder, who has granted medRxiv a license to display the preprint in perpetuity.\\nis the (which was not peer-reviewed) The copyright holder for this preprint . https://doi.org/10.1101/2020.02.05.20020107 doi: medRxiv preprint smokers who have quit for longer durations (>15 years). This may indicate a difference in ACE2 gene expression between Caucasian and Asian current smokers, but it is not statistically significantly detected due to the limited sample size in this study (p-value=0.43).\\nCompared to non-smokers, multivariate analysis on all data showed a significant higher ACE2 expression in former smokers (p-value=0.04) and a higher mean of ACE2 expression in current smokers, which did not reach statistical significance from current analysis though (p-value=0.11).\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enriched_covid_df.sort_values(by='smoking_rank', ascending=False).iloc[0]['text_body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_covid_df.to_json(\"../../enriched_covid_df.json\", orient='records')\n",
    "enriched_covid_df.to_csv(\"../../enriched_covid_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This marks the end of phase 1. The resulting dataframe will be stored as a json to be served by the web app."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Search (Phase 2)\n",
    "We will extend the capabilities from the `PaperAnalyzer` class and attempt to answer some questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `Question` class will decompose and resolve a question about risk factors.\n",
    "The result will then be piped to an instance of `PaperAnalyzer` to conduct similar analysis. Users will be able to specify their own list of outcomes. If not specified, the default set of outcomes will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Question():\n",
    "    \"\"\"\n",
    "    The purpose of this class is to resolve a question for \n",
    "    keyword searching\n",
    "    \"\"\"\n",
    "    def __init__(self, question, design_list=None, outcomes=None):\n",
    "        \"\"\"\n",
    "        The constuctor does most of the method-calling for question resolution\n",
    "        \"\"\"\n",
    "        self.DESIGN_LIST = design_list\n",
    "        self.OUTCOMES = outcomes\n",
    "        self.RISK = question\n",
    "        self.risk_factors = None\n",
    "        self.design_list = None\n",
    "        self.outcome_list = None\n",
    "        self.__resolve_question()\n",
    "        if design_list:\n",
    "            self.__resolve_design()\n",
    "        if outcomes:\n",
    "            self.__resolve_outcomes()\n",
    "\n",
    "    def __question_tokenize(self, sent):\n",
    "        \"\"\"\n",
    "        Cleans the question string\n",
    "        \"\"\"\n",
    "        abbvr_pattern = re.compile('(e.g.|i.e.)')\n",
    "        sent = abbvr_pattern.sub('', sent)\n",
    "        remove_punct_dict = {key: \" \" for key in string.punctuation}\n",
    "        remove_punct_dict['.'] = ''\n",
    "        remove_punct = str.maketrans(remove_punct_dict)\n",
    "        sent = sent.translate(remove_punct)\n",
    "        return sent.replace('R', 'R0').replace('-', ' ')\n",
    "\n",
    "    def __resolve_question(self):\n",
    "        \"\"\"\n",
    "        stems and removes irreleavnt words from questions\n",
    "        to create keywords for keyword analysis\n",
    "        \"\"\"\n",
    "        subquestion = self.RISK\n",
    "        sub_q = self.__question_tokenize(subquestion)\n",
    "        keywords = set([\n",
    "            stemmer.stem(word) for word in word_tokenize(sub_q)\n",
    "            if word.lower() not in stop_words and 'cov' not in word.lower()\n",
    "            and word.lower().islower()  #This checks and removes numbers\n",
    "        ])\n",
    "        self.risk_factors = list(keywords)\n",
    "\n",
    "    def __resolve_design(self):\n",
    "        \"\"\"\n",
    "        Resolves study designs to allow for study-design evaluation\n",
    "        \"\"\"\n",
    "        design_keys = self.DESIGN_LIST.split(\",\")\n",
    "        self.design_list = list(set(design_keys))\n",
    "\n",
    "    def __resolve_outcomes(self):\n",
    "        \"\"\"\n",
    "        Resolves outcomes to allow for outcome evaluation\n",
    "        \"\"\"\n",
    "        outcome_keys = self.__question_tokenize(self.OUTCOMES)\n",
    "        outcome_keys = set([\n",
    "            stemmer.stem(word) for word in word_tokenize(outcome_keys)\n",
    "            if word.lower() not in stop_words\n",
    "            and word.lower().islower()  #This checks and removes numbers\n",
    "        ])\n",
    "        self.outcome_list = list(outcome_keys)\n",
    "\n",
    "    def get_keywords(self):\n",
    "        \"\"\"\n",
    "        Returns keywords from earlier methods\n",
    "        \"\"\"\n",
    "        result = {'risk': None, 'design': None, 'outcome': None}\n",
    "        result['risk'] = self.risk_factors\n",
    "        if self.design_list:\n",
    "            result['design'] = self.design_list\n",
    "        if self.outcome_list:\n",
    "            result['outcome'] = self.outcome_list\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_question(df, question):\n",
    "    \"\"\"\n",
    "    Function to take in a Question instance and a \n",
    "    dataframe with covid-excerpts to perform \n",
    "    evaluation and rankings on information relevancy\n",
    "    \"\"\"\n",
    "    reference_df = PaperAnalyzer(df)\n",
    "    keys = question.get_keywords()\n",
    "    reference_df.perform_analysis(keys['risk'], keys['design'],\n",
    "                                  keys['outcome'])\n",
    "    return reference_df.get_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will be trying out the question answering pipeline now with a few questions from the aforementionned [medical dictionary](https://docs.google.com/spreadsheets/d/1t2e3CHGxHJBiFgHeW0dfwtvCG4x0CDCzcTFX7yz9Z2E/edit#gid=1217643351)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_google_sheet(url, sheet_name):\n",
    "    response=requests.get(url=url)\n",
    "    sample_file = io.BytesIO(response.content)\n",
    "    df = pd.read_excel(sample_file, sheet_name = sheet_name)\n",
    "    return df\n",
    "\n",
    "dict_url = 'https://docs.google.com/spreadsheets/d/1t2e3CHGxHJBiFgHeW0dfwtvCG4x0CDCzcTFX7yz9Z2E/export?format=xlsx&id=1t2e3CHGxHJBiFgHeW0dfwtvCG4x0CDCzcTFX7yz9Z2E'\n",
    "questions_df = get_google_sheet(dict_url, 'sub.question.matching')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Question</th>\n",
       "      <th>Subquestion</th>\n",
       "      <th>Outcome.list</th>\n",
       "      <th>Differences.list</th>\n",
       "      <th>Design.list</th>\n",
       "      <th>Notes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is known about transmission, incubation, ...</td>\n",
       "      <td>Range of incubation periods for the disease in...</td>\n",
       "      <td>Range of incubation periods for humans: genera...</td>\n",
       "      <td>incubation period</td>\n",
       "      <td>age.</td>\n",
       "      <td>contact tracing, survival analysis, time-to-ev...</td>\n",
       "      <td>https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is known about transmission, incubation, ...</td>\n",
       "      <td>Range of incubation periods for the disease in...</td>\n",
       "      <td>Range of incubation periods for humans: by age...</td>\n",
       "      <td>incubation period, stratified by age group</td>\n",
       "      <td>age.</td>\n",
       "      <td>contact tracing, survival analysis, time-to-ev...</td>\n",
       "      <td>Notes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Unnamed: 0  \\\n",
       "0  What is known about transmission, incubation, ...   \n",
       "1  What is known about transmission, incubation, ...   \n",
       "\n",
       "                                            Question  \\\n",
       "0  Range of incubation periods for the disease in...   \n",
       "1  Range of incubation periods for the disease in...   \n",
       "\n",
       "                                         Subquestion  \\\n",
       "0  Range of incubation periods for humans: genera...   \n",
       "1  Range of incubation periods for humans: by age...   \n",
       "\n",
       "                                 Outcome.list Differences.list  \\\n",
       "0                           incubation period             age.   \n",
       "1  incubation period, stratified by age group             age.   \n",
       "\n",
       "                                         Design.list  \\\n",
       "0  contact tracing, survival analysis, time-to-ev...   \n",
       "1  contact tracing, survival analysis, time-to-ev...   \n",
       "\n",
       "                                               Notes  \n",
       "0  https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4...  \n",
       "1                                              Notes  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is COVID-19 transmitted on droplets?'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Picking a sample question for analysis\n",
    "ques = questions_df.iloc[127]['Subquestion']\n",
    "ques = sent_tokenize(ques)[0]\n",
    "ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'risk factor analysis, logistic regression, cross-sectional case-control, matched case-control, observational case series, time series analysis, survival analysis'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Designs recommended for sample question\n",
    "des = questions_df.iloc[127]['Design.list']\n",
    "des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'odds of COVID-19 acquisition by occupation, age group, PPE use, observed/self-reported risk behaviors (e.g., inappropriately lowering mask to speak, touching face, eating without washing hands)'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Outcomes recommended for sample question\n",
    "outc = questions_df.iloc[127]['Outcome.list']\n",
    "outc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing risks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff020ad088434f2b9dee13b891d6ca24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing study designs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f17b7a1c67c4ca1bc3ba869bf67cc41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=304.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing outcomes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398e9b45de8f4f4f8fbadacfc2c52d08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=304.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analyzing fatality\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5146a1e124347199ed061ded6a84570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=304.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating final rankings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13cdf9ff3e444c07a738eb5eb443db3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "report_df = analyze_question(covid_df, Question(ques, des, outc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will now look at the 5 most relevant excerpts to the question we picked earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intubating a patient with COVID-19 is a high-risk procedure, due to the proximity of the health care workers to the patients' oropharynx and the exposure to airway secretions, which can carry a high viral load. 47 During the SARS outbreak in 2003, health care workers performing intubations were shown to be at a significantly increased risk of nosocomial transmission. 48 This risk was shown to be greatly reduced where PPE was used appropriately and infection control measures were followed. 10 The availability and suitability of facemasks and respirators has escalated into an emotive, as well as scientific debate. A fluid resistant surgical facemask protects the wearer against sprays of bodily fluids and large droplets, whereas N95, FFP2 and FFP3 respirators are thought to protect the wearer against aerosolised and airborne pathogens as well. In laboratory studies, a FFP2 mask filters at least 94% of all particles that are 0.3 microns in diameter or larger; N95 masks block at least 95%, \n",
      "----------------------------- \n",
      "\n",
      "While it is important to know which cytology specimens can contain viable and therefore transmissible virus, it is important to emphasize the use of universal standard precautions when dealing with any cytology specimen. From the experience with SARS we can extrapolate that SARS-CoV-2 can be present in fecal and urine samples, in addition to peripheral blood and respiratory samples. SARS-CoV-2 may be present in samples in patients without known COVID-19, in undiagnosed patients, in presymptomatic patients, in patients with asymptomatic or minimally symptomatic infections, and in convalescent patients, who may still be shedding the virus. From the available data it is very likely that many, if not most, infections occur through contact with individuals who either don't have the disease or were not diagnosed with COVID-19. In addition, as with most specimens, the clinical diagnosis may not be indicated in the requisition. For all these reasons, all fresh specimens should be considered po\n",
      "----------------------------- \n",
      "\n",
      "The novel nature of the COVID-19 pandemic presents many challenges to gynecologists working to optimize the care of their patients. No part of our practice has been more disrupted than the ability to offer timely procedures to our patients with gynecologic surgical indications.\n",
      "Despite efforts to limit elective surgery during the COVID-19 pandemic, urgent and emergent procedures must still be performed. For many of these urgent procedures, such as ruptured ectopic pregnancy, hemorrhagic ovarian cysts, or ovarian torsion, laparoscopy may offer the best surgical approach and outcomes for the patient. However, some have suggested that laparoscopic procedures should be avoided, and laparotomy procedures be universally employed when a patient is COVID-19 positive or status is unknown due to concerns about aerosolization of viral particles via the pneumoperitoneum. We strongly disagree with this premise. Based on best available evidence, we concur with both the recent U.S. joint professional\n",
      "----------------------------- \n",
      "\n",
      "Coronaviruses are enveloped positive sense RNA viruses ranging from 60 nm to 140 nm in diameter with spike like projections on its surface giving it a crown like appearance under the electron microscope; hence the name coronavirus [3] . Four corona viruses namely HKU1, NL63, 229E and OC43 have been in circulation in humans, and generally cause mild respiratory disease.\n",
      "There have been two events in the past two decades wherein crossover of animal betacorona viruses to humans has resulted in severe disease. The first such instance was in 2002-2003 when a new coronavirus of the β genera and with origin in bats crossed over to humans via the intermediary host of palm civet cats in the Guangdong province of China. This virus, designated as severe acute respiratory syndrome coronavirus affected 8422 people mostly in China and Hong Kong and caused 916 deaths (mortality rate 11%) before being contained [4] . Almost a decade later in 2012, the Middle East respiratory syndrome coronavirus (MERS\n",
      "----------------------------- \n",
      "\n",
      "Human to human transmission via either respiratory droplets or close contacts was initially proposed as the main routes of transmission of the pathogen based on experience gained in the previous two epidemics caused by coronaviruses (MERS-CoV and SARS-CoV) (8) . According to the world Health Organization (WHO) report, 2019-nCoV is a unique virus that causes respiratory disease, which spreads via oral and nasal droplets. Moreover, the pathogen of COVID-19 can float in the air in the form of aerosols and cause infection in healthy people (9) . Evidence of a study in Singapore revealed higher loads of virus in confirmed cases of COVID-19 in early stages of the disease, which decreased dramatically over time (10) . There is a limited number of evidence on oral-fecal transmissibility of the pathogen. However, COVID-19 RNA was found in fecal specimens of 2 to 10% of confirmed patients with gastrointestinal symptoms such as diarrhea (11, 12) , so fecal-oral transmission should be taken into a\n",
      "----------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5, 10):\n",
    "    print(report_df.sort_values(by='droplet_rank', ascending = False).iloc[i]['text_body'][:1000])\n",
    "    print(\"-----------------------------\",\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('anaconda3': virtualenv)",
   "language": "python",
   "name": "python37464bitanaconda3virtualenv11568a5b709c405b925f37b6b0b6dbdb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
